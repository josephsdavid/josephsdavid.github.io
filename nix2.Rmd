---
title: "Practical Nix for Data Science: Part 2"
author: "David Josephs"
date: "`r Sys.Date()`"
---
[Nix part 1](nix.html)

In this post, we will get into some very cool data science specific uses for Nix, as I have seen a few [questions on twitter related to machine learning software stacks](https://twitter.com/chipro/status/1202815757593108480?s=20). We will discuss A) building models reproducibly with nix, and hosting them on a [binary cache](https://cachix.org/) and B) integrating this with [continuous integration](https://hercules-ci.com/)

# Reproducibly Building Models

A prerequisite for this post is reading and understanding my [previous post on nix](nix.html), as we are about to get into some more complex stuff. Specifically read through the building and hosting on a binary cache section. In this case, we are going to learn to include source code that is not in nixpkgs, instead it will be our own source code, which trains a model! Lets get started. First, lets go over the directory structure for our project:

```sh
.
├── default.nix
├── model.nix
├── shell.nix
└── py
    └── trainer
        └── train.py
```

The important files here are `default.nix`, `model.nix`, and `py/*`. What we are going to do is build our project from the bottom up, starting with the trainer python file, then building a nix expression for it, then we build our default.nix, which compiles everything together. For full reproducibility, I will also demonstrate how to pin nixpkgs, for true determinism!  The `shell.nix` is optional and used for my development environment. Note, I will be following an example [from the tokyo nixos meetup](https://github.com/Tokyo-NixOS/presentations), except simplifying some things and overcomplicating others.

## Defining the Model

First, lets build a simple keras model:

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
import numpy as np
import os

class ModelTrainer:

    def __init__(self):
         # we will define these in model.nix, as build variables!
         self.epochs     = int( os.environ.get('EPOCHS', 12) )
         # Reproducible data!
         self.data_file  = os.environ.get('DATA')
         # where we save the model
         self.model_file = os.environ.get('MODEL', 'model.h5')

    def load_data(self, file):
        f = np.load(self.data_file)
        return (f['x_train'], f['y_train']), (f['x_test'], f['y_test'])

    def train(self):
        # input image dimensions
        img_rows, img_cols = 28, 28

        batch_size = 128
        num_classes = 10

        epochs = self.epochs

        (x_train, y_train), (x_test, y_test) = self.load_data(self.data_file)
        
        if K.image_data_format() == 'channels_first':
            x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
            x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
            input_shape = (1, img_rows, img_cols)
        else:
            x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
            x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
            input_shape = (img_rows, img_cols, 1)
        
        x_train = x_train.astype('float32')
        x_test = x_test.astype('float32')
        x_train /= 255
        x_test /= 255
        print('x_train shape:', x_train.shape)
        print(x_train.shape[0], 'train samples')
        print(x_test.shape[0], 'test samples')
        
        # convert class vectors to binary class matrices
        y_train = tf.keras.utils.to_categorical(y_train, num_classes)
        y_test = tf.keras.utils.to_categorical(y_test, num_classes)
        
        model = Sequential()
        model.add(Conv2D(32, kernel_size=(3, 3),
                         activation='relu',
                         input_shape=input_shape))
        model.add(Conv2D(64, (3, 3), activation='relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.25))
        model.add(Flatten())
        model.add(Dense(128, activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(num_classes, activation='softmax'))
        
        model.compile(loss="categorical_crossentropy",
                      optimizer="adam",
                      metrics=['accuracy'])
        
        model.fit(x_train, y_train,
                  batch_size=batch_size,
                  epochs=epochs,
                  verbose=1,
                  validation_data=(x_test, y_test))
        score = model.evaluate(x_test, y_test, verbose=0)
        
        model.save(self.model_file)

        print('Test loss:', score[0])
        print('Test accuracy:', score[1])


mt = ModelTrainer()
mt.train()
```

As we can see nothing very complicated here, it is just [the example from the keras documentation](https://keras.io/examples/mnist_cnn/). The only thing different is we are letting a few of our training variables and data be environment variables. That way, we can change those at build time, these will be defined in model.nix. Now we can really get into it. 

## nix expression for the model

As usual, I will first display the code, and then walk through it line by line. Here is the file model.nix:

```nix
{ stdenv, fetchurl, python37Packages
# Number of epochs
, epochs ? 10
# Selecting backend
, cuda ? true
}:

with stdenv.lib;

let

/*
Model data!
*/
data = fetchurl {
  url    =  https://s3.amazonaws.com/img-datasets/mnist.npz;
  sha256 = "1lbknqbzqs44qhnczv9a5bfdjl5qqgwgrgwgwk4609vm0b35l73k";
};

in stdenv.mkDerivation rec {
  name    = "model-${version}";
  version = "1";

  src  =  ./py/trainer;

  /* Model dependencies, adapted to the backed
  */
  nativeBuildInputs = with python37Packages; [
    python
    h5py
  ] ++ optionals (cuda == true) [
    python37Packages.tensorflowWithCuda
  ] ++ optionals (cuda == false) [
    python37Packages.tensorflow
  ];

  /* requiredSystemFeatures could be used to make
     the model leverage nix distributed builds

       requiredSystemFeatures = [ "big-parallel" "cuda" ];
  */

  /* environment variables used in the train script
  */
  EPOCHS = epochs;
  DATA = data;

  unpackPhase = ":";

  /* Training the model
  */
  buildPhase = ''
    python $src/train.py
  '';

  installPhase = ''
    mkdir -p $out
    cp model.h5 $out/
  '';
}
```

So lets go through this line by line

### Whats in those brackets?

This is our first time making a nix expression that doesnt do anything on its own. If you remember, in our previous example, we made an expression to include something that wasnt from nixpkgs in the same file as our build file. In this case, we are building something a bit bigger than a simple plotting package, so we will, for our mental health, want to make things modular. We can view this entire `model.nix` file as a function, where the stuff in the brackets at the top is **arguments to the function**. A question marks indicates a default option. So, word by word, the first argument is `stdenv`, which is nix's standard set of build tools. Next is `fetchurl`, which allows us to get information from a website. We thin indicate which set of python packages we want (`python37Packages`). We are then specifying a default of 10 epochs (`epochs ? 10`) and that by default the model will use cuda (`cuda ? true`).

### Getting Data

Next, we put everything under the scope of nix's stdenv (build libs) (`with stdenv.lib`). This is pretty much just boilerplate, we need the build library to build things!! We then define the data variable. `fetchurl` is a function which requires two arguments in this case, it requires the url (unquoted), and a SHA hash to verify we are getting the right thing. We can either get this hash with `nix-prefetch-url` on the command line or we can just put in the wrong hash and copy and paste in the error output (not advised or particularly secure, but as a lazy master's student I do this all the time).

### Making a derivation

We next call the `stdenv.mkDerivation` function, which takes in a recursive set (`rec`). The first thing we specify is the name of our derivation, which in this case is `model-${version}`. Nix will take the version we give later, and append it to the name. Then, when our model is built, it will appear in /nix/store as `longhash-model-${version}`, and the same in our binary cache. Next, and this is very important, we need to specify where the source code lives (`src`). Otherwise, Nix will not be able to build anything!! The source code for our model is in `src/trainer` (relative to where model.nix is).

### Specifying dependencies

Here, instead of `buildInputs`, we are using `nativeBuildInputs`. I was wondering why this was the case (they did this in the example), so I did a [bit of digging](https://matthewbauer.us/blog/beginners-guide-to-cross.html). Basically, when **building** a package, you should use `nativeBuildInputs` for your build time dependencies, these are dependencies which will only be around when the package is being built. After the package is built, these will go away, while with `buildInputs`, they will not. Since we are basically just compiling a model to an h5 file (or in the case of standard machine learning, whatever filetype the output of `joblib.dump` is :) ), we do not need whatever version of tensorflow or python or anything to persist. We just need the resulting model file! So it is better to use `nativeBuildInputs`. Apparently (and I cannot claim to know much about this), this is a best practice for cross-compilation for OSes other than NixOS.  Lets walk through our buildinputs now. First, we are putting the non optional ones under the scope of `python37Packages`. This means we are going to point everything in this `[]` list to the python37 section of nixpkgs, and look for them there. We have two things which are absolutely required for this: `h5py`, to save the model, and fairly obviously `python`.
Next, we specify our optional inputs by appending to the list. These are build time dependencies that depend on variables we set, in our case, the usage of CUDA. If we are going to use CUDA( `++ optionals (cuda == true)`), we are going to require tensorflowWithCuda. Note that the optionals are outside the scope of `with python37Packages`, so we need to include that. If we arent going to use CUDA ( `++ optionals (cuda == false)` ), we use the normal tensorflow.

### Specifying our environment variables

We next need to specify the environment variables called by our source python code. We do this very simply, with equal signs...

```nix
EPOCHS=epochs;
DATA=data;
```

If we wanted to specify a model file, we would do that here too

### Build and install

To build and install, we require 3 phases: the unpackPhase, the buildPhase, and the installPhase. The unpackPhase is to unpack any source code, which we do not need to do, so we set it to `":"`, which does nothing. We could alternative specify our own build phases, like this:

```nix
phases = ["buildPhase" "installPhase"];
```

And then completely leave out the unpack phase. In the buildPhase, we run our python code, and finally in the installPhase, we make an output directory, and put our resulting model in there!

If you got through this, congratulations! We are nearly set up!

# Building everything!

We have finished the hard part, now we just have to put it all together. Lets go ahead and look at our default.nix file:

```nix
with import <nixpkgs> {};

rec {
  /* Model using default values (cuda true, 10 epochs)
  */
  model = pkgs.callPackage ./model.nix {};

  #/* Model using tensorflow backend with 1 epoch
  #*/
  #simple-model = pkgs.callPackage ./model.nix {
  #  epochs = 1;
  #};

  #/* Model using no CUDA backend with 3 epochs
  #*/
  slow-model = pkgs.callPackage ./model.nix {
    cuda = False;
    epochs  = 20;
  };
}
``` 

And that is it! We are just going to use the callPackage function, which just runs everything. Note this structure, as if we had a ton of models we wanted built and saved, we could easily do that! Just with this exact same format. Note, currently, this is hardly reproducible. Another issue, if we run this code as is, we are going to be stuck using tf 1.15, which is not what we want. To fix this, we can actually pin nixpkgs. You can pin either with the fetchTarball or with fetchGit. We will do fetchGit, because I want a specific fork of nixpkgs which has tensorflow 2 specified:

```nix
with import (    
  builtins.fetchGit {
      name = "nixos-tensorflow-2";
      url = https://github.com/nixos/nixpkgs;
      ref = "d59b4d07045418bae85a9bdbfdb86d60bc1640bc";}) {};

rec {
  /* Model using default values (cuda true, 10 epochs)
  */
  model = pkgs.callPackage ./model.nix {};

  #/* Model using tensorflow backend with 1 epoch
  #*/
  #simple-model = pkgs.callPackage ./model.nix {
  #  epochs = 1;
  #};

  #/* Model using no CUDA backend with 3 epochs
  #*/
  slow-model = pkgs.callPackage ./model.nix {
    cuda = False;
    epochs  = 20;
  };
}
``` 
A very simple change, but this has tensorflow 2 intead of tensorflow 1.15, and will never change (as it is a specific git commit), but it will make our life easier for us and our collegues.
