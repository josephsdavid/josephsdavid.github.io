<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Josephs" />

<meta name="date" content="2019-12-08" />

<title>Interpretable Machine Learning Part 1</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/darkly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<link href="site_libs/ionicons-2.0.1/css/ionicons.min.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #232629;
    color: #7a7c7d;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #7a7c7d;  padding-left: 4px; }
div.sourceCode
  { color: #cfcfc2; background-color: #232629; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span. { color: #cfcfc2; } /* Normal */
code span.al { color: #95da4c; } /* Alert */
code span.an { color: #3f8058; } /* Annotation */
code span.at { color: #2980b9; } /* Attribute */
code span.bn { color: #f67400; } /* BaseN */
code span.bu { color: #7f8c8d; } /* BuiltIn */
code span.cf { color: #fdbc4b; } /* ControlFlow */
code span.ch { color: #3daee9; } /* Char */
code span.cn { color: #27aeae; } /* Constant */
code span.co { color: #7a7c7d; } /* Comment */
code span.cv { color: #7f8c8d; } /* CommentVar */
code span.do { color: #a43340; } /* Documentation */
code span.dt { color: #2980b9; } /* DataType */
code span.dv { color: #f67400; } /* DecVal */
code span.er { color: #da4453; } /* Error */
code span.ex { color: #0099ff; } /* Extension */
code span.fl { color: #f67400; } /* Float */
code span.fu { color: #8e44ad; } /* Function */
code span.im { color: #27ae60; } /* Import */
code span.in { color: #c45b00; } /* Information */
code span.kw { color: #cfcfc2; } /* Keyword */
code span.op { color: #cfcfc2; } /* Operator */
code span.ot { color: #27ae60; } /* Other */
code span.pp { color: #27ae60; } /* Preprocessor */
code span.re { color: #2980b9; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #da4453; } /* SpecialString */
code span.st { color: #f44f4f; } /* String */
code span.va { color: #27aeae; } /* Variable */
code span.vs { color: #da4453; } /* VerbatimString */
code span.wa { color: #da4453; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">David Does Data</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Nix
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="nix.html">Nix for Data Science 1</a>
    </li>
    <li>
      <a href="advancedNix.html">Nix for Data Science 2: COMING SOON</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    IML
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="iml.html">Interpretable Machine Learning: Part 1</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Time Series
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="tsEDA.html">Time Series EDA</a>
    </li>
    <li>
      <a href="tspreprocessing.html">Time Series Cleaning</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/josephsdavid">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="https://twitter.com/Daved256">
    <span class="ion ion-social-twitter"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Interpretable Machine Learning Part 1</h1>
<h4 class="author">David Josephs</h4>
<h4 class="date">2019-12-08</h4>

</div>


<style>
div.blue pre { background-color:#d8ecf2; }
div.blue pre.r { background-color:black; }
div.blue pre.python { background-color:black; }
</style>
<p>In this blog post, we are going to talk about a few tools</p>
<div id="permutation-importance-math-and-intuition" class="section level1">
<h1>Permutation Importance: Math and Intuition</h1>
<p>Everyone loves tree based models. Gradient boosting, random forests, and friends are wonderful, flexible tools. One of the other benefits of these models, because of their tree-ness, is that we are able to actually see how “important” each variable is in the decisions the model is making. This is also one of many many reasons why we love linear models, we can actually see and quantify the strength of a feature in our model. However, why must we limit ourselves to just linear and tree based models?</p>
<p>Lets try and think of a new approach to get variable importance. When I was first really getting into ML, I remember asking one of my professors the question: “How much time do you spend on feature engineering?”. I will never forget his answer, he told me: “Feature engineering [is] the most crucial part to improve both accuracy and model generation. If you have an unneccessary feature in the model, you are in essence fitting noise.”. This has stuck with me for a long time, and it is a useful thing to keep in mind while discussing permutation importance.</p>
<p>If unneccessary features just provide noise which decreases model accuracy and generalization, what happens if we replace a good feature with noise? Our model should be inherently worse, no? This is the key idea of permutation importance.</p>
<blockquote>
<p>If I replace a feature with noise, how much worse does the model perform?</p>
</blockquote>
<p>This is the key idea of permutation based variable importance. All we are going to do to calculate this is three simple steps:</p>
<ol style="list-style-type: decimal">
<li><p>Calculate prediction loss</p></li>
<li><p>Replace a feature with noise</p></li>
<li><p>Recalculate prediction loss</p></li>
<li><p>Compare</p></li>
</ol>
<div id="formalization" class="section level2">
<h2>Formalization</h2>
<p>Lets formulate permutation importance mathematically now! First, lets define our data as the set <span class="math inline">\(x\)</span>, with <span class="math inline">\(m\)</span> observations and <span class="math inline">\(n\)</span> features. Next, lets consider two sets within <span class="math inline">\(x\)</span>: <span class="math inline">\(x_s\)</span> and <span class="math inline">\(x_c\)</span>. <span class="math inline">\(x_s\)</span> represents the feature(s) we are interested in, and <span class="math inline">\(x_c\)</span> represents the complement of <span class="math inline">\(x_s\)</span> (in english, everything else). Thus:</p>
<p><span class="math display">\[x = (x_s, x_c)\]</span></p>
<p>Lets first define the original loss with the original features as <span class="math inline">\(\mathcal{L}\)</span>,</p>
<p><span class="math display">\[
\mathcal{L} = \mathrm{loss}\left(x_s, x_c \right) 
\]</span></p>
<p>Next, we need to replace <span class="math inline">\(x_s\)</span> with noise. To do that, we want to sample the <em>marginal distribtion</em> of <span class="math inline">\(x_s\)</span>. This means we want to sample the distribution of <span class="math inline">\(x_s\)</span> <em>independent of other features</em>. With a reasonably sized dataset, we can just do a permutation of <span class="math inline">\(x_s\)</span> for more or less the same result. We will denote the permutation of <span class="math inline">\(x_s\)</span> as <span class="math inline">\(x_s^*\)</span>. Next, lets define the loss, <span class="math inline">\(\mathcal{L}*\)</span> of the permuted feature: <span class="math display">\[
\mathcal{L}* = \mathrm{loss}\left(x_s^*, x_c \right)
\]</span></p>
<p>Finally, we can calculate the variable importance of <span class="math inline">\(x_s\)</span>:</p>
<p><span class="math display">\[
VIP_{\mathrm{perm}}(x_s) = \frac{\mathcal{L}*}{\mathcal{L}}
\]</span></p>
<p>There we go! Its that simple! An addendum to this suggested by Jeremy Howard of fast.ai: Add a feature of pure noise and see how important that is, for reference.</p>
</div>
<div id="code-implementation" class="section level2 tabset tabset-pills">
<h2>Code Implementation</h2>
<p>For our data, we will use the dataset discussed in <a href="https://medium.com/towards-artificial-intelligence/training-a-machine-learning-model-on-a-dataset-with-highly-correlated-features-debddf5b2e34">Benjamin Tayo’s amazing blog post</a>. We use this dataset because it presents a large challenge to us, with highly correlated features. This will show some of the pitfalls of some of our techniques.</p>
<p>The goal with this dataset is to predict the number of crew members which will be on a cruise ship, given some paramters describing the ship. I believe the independent variables are fairly self explanatory. First, lets read the data into python and do a train test split:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsRegressor</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_boston, fetch_california_housing</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error <span class="im">as</span> loss_mse</span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="im">import</span> math</span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="im">import</span> statistics <span class="im">as</span> stats</span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="im">import</span> matplotlib.cm <span class="im">as</span> cm</span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="im">from</span> pprint <span class="im">import</span> pprint</span>
<span id="cb1-14"><a href="#cb1-14"></a></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co"># so this is readable:</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="im">import</span> warnings</span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="im">from</span> sklearn.exceptions <span class="im">import</span> DataConversionWarning, ConvergenceWarning</span>
<span id="cb1-18"><a href="#cb1-18"></a>warnings.filterwarnings(action<span class="op">=</span><span class="st">&#39;ignore&#39;</span>, category<span class="op">=</span>DataConversionWarning)</span>
<span id="cb1-19"><a href="#cb1-19"></a>warnings.simplefilter(action<span class="op">=</span><span class="st">&#39;ignore&#39;</span>, category<span class="op">=</span><span class="pp">FutureWarning</span>)</span>
<span id="cb1-20"><a href="#cb1-20"></a>warnings.simplefilter(action<span class="op">=</span><span class="st">&#39;ignore&#39;</span>, category<span class="op">=</span>ConvergenceWarning)</span>
<span id="cb1-21"><a href="#cb1-21"></a>cruise <span class="op">=</span> pd.read_csv(<span class="st">&quot;https://github.com/bot13956/ML_Model_for_Predicting_Ships_Crew_Size/raw/master/cruise_ship_info.csv&quot;</span>)</span>
<span id="cb1-22"><a href="#cb1-22"></a></span>
<span id="cb1-23"><a href="#cb1-23"></a></span>
<span id="cb1-24"><a href="#cb1-24"></a>X <span class="op">=</span> cruise.loc[:, cruise.columns <span class="op">!=</span> <span class="st">&quot;crew&quot;</span>]</span>
<span id="cb1-25"><a href="#cb1-25"></a>X <span class="op">=</span> X.loc[:, X.columns <span class="op">!=</span> <span class="st">&quot;Ship_name&quot;</span>]</span>
<span id="cb1-26"><a href="#cb1-26"></a>X <span class="op">=</span> X.loc[:, X.columns <span class="op">!=</span> <span class="st">&quot;Cruise_line&quot;</span>]</span>
<span id="cb1-27"><a href="#cb1-27"></a>y <span class="op">=</span> cruise.loc[:, cruise.columns <span class="op">==</span> <span class="st">&quot;crew&quot;</span>]</span>
<span id="cb1-28"><a href="#cb1-28"></a></span>
<span id="cb1-29"><a href="#cb1-29"></a></span>
<span id="cb1-30"><a href="#cb1-30"></a><span class="kw">def</span> split(df, p_train <span class="op">=</span> <span class="fl">0.75</span>, random_state <span class="op">=</span> <span class="dv">0</span>):</span>
<span id="cb1-31"><a href="#cb1-31"></a>    train <span class="op">=</span> df.sample(frac <span class="op">=</span> p_train, random_state <span class="op">=</span> random_state)</span>
<span id="cb1-32"><a href="#cb1-32"></a>    test <span class="op">=</span> df.drop(train.index)</span>
<span id="cb1-33"><a href="#cb1-33"></a>    <span class="cf">return</span>(train, test)</span>
<span id="cb1-34"><a href="#cb1-34"></a></span>
<span id="cb1-35"><a href="#cb1-35"></a>(X_train, X_test), (y_train, y_test) <span class="op">=</span> (split(x) <span class="cf">for</span> x <span class="kw">in</span> [X, y])</span></code></pre></div>
<p>Next, lets use the amazing <code>reticulate</code> package to pass these exact data frames into R:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>X &lt;-<span class="st"> </span>py<span class="op">$</span>X</span>
<span id="cb2-2"><a href="#cb2-2"></a>y &lt;-<span class="st"> </span>py<span class="op">$</span>y</span>
<span id="cb2-3"><a href="#cb2-3"></a>X_train &lt;-<span class="st"> </span>py<span class="op">$</span>X_train</span>
<span id="cb2-4"><a href="#cb2-4"></a>X_test &lt;-<span class="st"> </span>py<span class="op">$</span>X_test</span>
<span id="cb2-5"><a href="#cb2-5"></a>y_train &lt;-<span class="st"> </span>py<span class="op">$</span>y_train</span>
<span id="cb2-6"><a href="#cb2-6"></a>y_test &lt;-<span class="st"> </span>py<span class="op">$</span>y_test</span>
<span id="cb2-7"><a href="#cb2-7"></a>X</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Age"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Tonnage"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["passengers"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["length"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["cabins"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["passenger_density"],"name":[6],"type":["dbl"],"align":["right"]}],"data":[{"1":"6","2":"30.277","3":"6.94","4":"5.94","5":"3.55","6":"42.64"},{"1":"6","2":"30.277","3":"6.94","4":"5.94","5":"3.55","6":"42.64"},{"1":"26","2":"47.262","3":"14.86","4":"7.22","5":"7.43","6":"31.80"},{"1":"11","2":"110.000","3":"29.74","4":"9.53","5":"14.88","6":"36.99"},{"1":"17","2":"101.353","3":"26.42","4":"8.92","5":"13.21","6":"38.36"},{"1":"22","2":"70.367","3":"20.52","4":"8.55","5":"10.20","6":"34.29"},{"1":"15","2":"70.367","3":"20.52","4":"8.55","5":"10.20","6":"34.29"},{"1":"23","2":"70.367","3":"20.56","4":"8.55","5":"10.22","6":"34.23"},{"1":"19","2":"70.367","3":"20.52","4":"8.55","5":"10.20","6":"34.29"},{"1":"6","2":"110.239","3":"37.00","4":"9.51","5":"14.87","6":"29.79"},{"1":"10","2":"110.000","3":"29.74","4":"9.51","5":"14.87","6":"36.99"},{"1":"28","2":"46.052","3":"14.52","4":"7.27","5":"7.26","6":"31.72"},{"1":"18","2":"70.367","3":"20.52","4":"8.55","5":"10.20","6":"34.29"},{"1":"17","2":"70.367","3":"20.52","4":"8.55","5":"10.20","6":"34.29"},{"1":"11","2":"86.000","3":"21.24","4":"9.63","5":"10.62","6":"40.49"},{"1":"8","2":"110.000","3":"29.74","4":"9.51","5":"14.87","6":"36.99"},{"1":"9","2":"88.500","3":"21.24","4":"9.63","5":"10.62","6":"41.67"},{"1":"15","2":"70.367","3":"20.52","4":"8.55","5":"10.20","6":"34.29"},{"1":"12","2":"88.500","3":"21.24","4":"9.63","5":"11.62","6":"41.67"},{"1":"20","2":"70.367","3":"20.52","4":"8.55","5":"10.20","6":"34.29"},{"1":"12","2":"88.500","3":"21.24","4":"9.63","5":"10.56","6":"41.67"},{"1":"14","2":"101.509","3":"27.58","4":"8.93","5":"13.21","6":"36.81"},{"1":"9","2":"110.000","3":"29.74","4":"9.52","5":"14.87","6":"36.99"},{"1":"13","2":"101.509","3":"27.58","4":"8.93","5":"13.79","6":"36.81"},{"1":"18","2":"70.606","3":"17.70","4":"8.15","5":"8.75","6":"39.89"},{"1":"11","2":"91.000","3":"20.32","4":"9.65","5":"9.75","6":"44.78"},{"1":"17","2":"77.713","3":"18.90","4":"8.66","5":"9.35","6":"41.12"},{"1":"12","2":"91.000","3":"20.32","4":"9.65","5":"9.75","6":"44.78"},{"1":"16","2":"77.713","3":"18.82","4":"8.66","5":"9.35","6":"41.29"},{"1":"13","2":"91.000","3":"20.32","4":"9.65","5":"9.75","6":"44.78"},{"1":"5","2":"122.000","3":"28.50","4":"10.33","5":"6.87","6":"34.57"},{"1":"12","2":"91.000","3":"20.32","4":"9.65","5":"9.75","6":"44.78"},{"1":"12","2":"2.329","3":"0.94","4":"2.96","5":"0.45","6":"24.78"},{"1":"21","2":"47.225","3":"13.66","4":"6.82","5":"6.87","6":"34.57"},{"1":"21","2":"28.430","3":"8.08","4":"6.16","5":"4.10","6":"35.19"},{"1":"13","2":"85.619","3":"21.14","4":"9.57","5":"10.56","6":"40.50"},{"1":"22","2":"52.926","3":"13.02","4":"7.18","5":"6.54","6":"40.65"},{"1":"27","2":"53.872","3":"14.94","4":"7.98","5":"7.67","6":"36.06"},{"1":"10","2":"105.000","3":"27.20","4":"8.90","5":"13.56","6":"38.60"},{"1":"9","2":"105.000","3":"27.20","4":"8.90","5":"13.56","6":"38.60"},{"1":"23","2":"25.000","3":"7.76","4":"6.22","5":"3.86","6":"32.22"},{"1":"10","2":"86.000","3":"21.14","4":"9.60","5":"10.56","6":"40.68"},{"1":"20","2":"53.049","3":"13.44","4":"7.22","5":"6.78","6":"39.47"},{"1":"6","2":"112.000","3":"38.00","4":"9.51","5":"15.00","6":"29.47"},{"1":"17","2":"75.166","3":"19.28","4":"8.28","5":"9.64","6":"38.99"},{"1":"10","2":"68.000","3":"10.80","4":"7.90","5":"5.50","6":"62.96"},{"1":"18","2":"51.004","3":"9.40","4":"7.81","5":"4.80","6":"54.26"},{"1":"44","2":"70.327","3":"17.91","4":"9.63","5":"9.50","6":"39.27"},{"1":"10","2":"151.400","3":"26.20","4":"11.32","5":"11.34","6":"57.79"},{"1":"6","2":"90.000","3":"20.00","4":"9.64","5":"10.29","6":"45.00"},{"1":"15","2":"83.338","3":"17.50","4":"9.64","5":"8.75","6":"47.62"},{"1":"14","2":"83.000","3":"17.50","4":"9.64","5":"8.75","6":"47.43"},{"1":"13","2":"61.000","3":"13.80","4":"7.80","5":"6.88","6":"44.20"},{"1":"5","2":"86.000","3":"21.04","4":"9.36","5":"10.22","6":"40.87"},{"1":"20","2":"55.451","3":"12.64","4":"7.19","5":"6.32","6":"43.87"},{"1":"29","2":"33.920","3":"12.14","4":"7.04","5":"6.07","6":"27.94"},{"1":"10","2":"81.769","3":"18.48","4":"9.59","5":"9.24","6":"44.25"},{"1":"25","2":"38.000","3":"7.49","4":"6.74","5":"3.96","6":"50.73"},{"1":"16","2":"59.652","3":"13.20","4":"7.77","5":"6.60","6":"45.19"},{"1":"19","2":"55.451","3":"12.66","4":"7.19","5":"6.33","6":"43.80"},{"1":"20","2":"55.451","3":"12.66","4":"7.19","5":"6.33","6":"43.80"},{"1":"17","2":"55.451","3":"12.66","4":"7.19","5":"6.33","6":"43.80"},{"1":"14","2":"63.000","3":"14.40","4":"7.77","5":"7.20","6":"43.75"},{"1":"27","2":"53.872","3":"14.94","4":"7.98","5":"7.47","6":"36.06"},{"1":"13","2":"63.000","3":"14.40","4":"7.77","5":"7.20","6":"43.75"},{"1":"11","2":"85.000","3":"18.48","4":"9.51","5":"9.24","6":"46.00"},{"1":"12","2":"58.600","3":"15.66","4":"8.24","5":"7.83","6":"37.42"},{"1":"5","2":"133.500","3":"39.59","4":"10.93","5":"16.37","6":"33.72"},{"1":"10","2":"58.825","3":"15.60","4":"8.23","5":"7.65","6":"37.71"},{"1":"31","2":"35.143","3":"12.50","4":"6.69","5":"5.32","6":"28.11"},{"1":"7","2":"89.600","3":"25.50","4":"9.61","5":"12.75","6":"35.14"},{"1":"9","2":"59.058","3":"17.00","4":"7.63","5":"8.50","6":"34.74"},{"1":"36","2":"16.852","3":"9.52","4":"5.41","5":"3.83","6":"17.70"},{"1":"11","2":"58.600","3":"15.66","4":"8.23","5":"7.83","6":"37.42"},{"1":"25","2":"34.250","3":"10.52","4":"6.15","5":"5.26","6":"32.56"},{"1":"11","2":"90.000","3":"22.40","4":"9.65","5":"11.20","6":"40.18"},{"1":"21","2":"50.760","3":"17.48","4":"7.54","5":"8.74","6":"29.04"},{"1":"6","2":"93.000","3":"23.94","4":"9.65","5":"11.97","6":"38.85"},{"1":"8","2":"91.000","3":"22.44","4":"9.65","5":"11.22","6":"40.55"},{"1":"21","2":"38.000","3":"10.56","4":"5.67","5":"5.28","6":"35.98"},{"1":"14","2":"77.104","3":"20.02","4":"8.53","5":"10.01","6":"38.51"},{"1":"9","2":"81.000","3":"21.44","4":"9.21","5":"10.72","6":"37.78"},{"1":"25","2":"42.000","3":"15.04","4":"7.08","5":"7.52","6":"27.93"},{"1":"15","2":"75.338","3":"19.56","4":"8.79","5":"9.83","6":"38.52"},{"1":"40","2":"28.000","3":"11.50","4":"6.74","5":"4.00","6":"24.35"},{"1":"12","2":"77.104","3":"20.02","4":"8.53","5":"10.01","6":"38.51"},{"1":"20","2":"50.760","3":"17.48","4":"7.54","5":"8.74","6":"29.04"},{"1":"15","2":"30.277","3":"6.84","4":"5.94","5":"3.42","6":"44.26"},{"1":"13","2":"30.277","3":"6.84","4":"5.94","5":"3.42","6":"44.26"},{"1":"15","2":"30.277","3":"6.84","4":"5.94","5":"3.42","6":"44.26"},{"1":"48","2":"22.080","3":"8.26","4":"5.78","5":"4.25","6":"26.73"},{"1":"9","2":"85.000","3":"19.68","4":"9.35","5":"9.84","6":"43.19"},{"1":"29","2":"45.000","3":"11.78","4":"7.54","5":"5.30","6":"38.20"},{"1":"13","2":"76.000","3":"18.74","4":"8.86","5":"9.39","6":"40.55"},{"1":"10","2":"77.000","3":"20.16","4":"8.56","5":"9.75","6":"38.19"},{"1":"18","2":"69.153","3":"18.82","4":"8.53","5":"9.14","6":"36.74"},{"1":"5","2":"115.000","3":"35.74","4":"9.00","5":"15.32","6":"32.18"},{"1":"9","2":"116.000","3":"26.00","4":"9.51","5":"13.00","6":"44.62"},{"1":"11","2":"91.627","3":"19.74","4":"9.64","5":"9.87","6":"46.42"},{"1":"7","2":"116.000","3":"31.00","4":"9.51","5":"15.57","6":"37.42"},{"1":"16","2":"77.499","3":"19.50","4":"8.56","5":"10.50","6":"39.74"},{"1":"9","2":"113.000","3":"26.74","4":"9.51","5":"13.37","6":"42.26"},{"1":"6","2":"113.000","3":"37.82","4":"9.51","5":"15.57","6":"29.88"},{"1":"12","2":"108.865","3":"27.58","4":"9.51","5":"13.00","6":"39.47"},{"1":"15","2":"108.806","3":"26.00","4":"9.51","5":"13.00","6":"41.85"},{"1":"10","2":"91.627","3":"19.74","4":"9.64","5":"9.87","6":"46.42"},{"1":"14","2":"30.277","3":"6.86","4":"5.93","5":"3.44","6":"44.14"},{"1":"22","2":"69.845","3":"15.90","4":"8.03","5":"7.95","6":"43.93"},{"1":"29","2":"44.348","3":"12.00","4":"7.54","5":"6.00","6":"36.96"},{"1":"9","2":"113.000","3":"26.74","4":"9.51","5":"13.37","6":"42.26"},{"1":"8","2":"77.499","3":"19.50","4":"8.56","5":"9.75","6":"39.74"},{"1":"11","2":"108.977","3":"26.02","4":"9.51","5":"13.01","6":"41.88"},{"1":"18","2":"77.499","3":"19.50","4":"8.56","5":"9.75","6":"39.74"},{"1":"14","2":"30.277","3":"6.88","4":"5.93","5":"3.44","6":"44.01"},{"1":"27","2":"12.500","3":"3.94","4":"4.36","5":"0.88","6":"31.73"},{"1":"12","2":"50.000","3":"7.00","4":"7.09","5":"3.54","6":"71.43"},{"1":"14","2":"33.000","3":"4.90","4":"5.60","5":"2.45","6":"67.35"},{"1":"16","2":"19.200","3":"3.20","4":"5.13","5":"1.60","6":"60.00"},{"1":"10","2":"46.000","3":"7.00","4":"6.70","5":"1.82","6":"65.71"},{"1":"12","2":"138.000","3":"31.14","4":"10.20","5":"15.57","6":"44.32"},{"1":"11","2":"90.090","3":"25.01","4":"9.62","5":"10.50","6":"36.02"},{"1":"23","2":"48.563","3":"20.20","4":"6.92","5":"8.00","6":"24.04"},{"1":"16","2":"74.137","3":"19.50","4":"9.16","5":"9.75","6":"38.02"},{"1":"13","2":"138.000","3":"31.14","4":"10.20","5":"15.57","6":"44.32"},{"1":"7","2":"158.000","3":"43.70","4":"11.12","5":"18.00","6":"36.16"},{"1":"17","2":"74.137","3":"19.50","4":"9.16","5":"9.75","6":"38.02"},{"1":"5","2":"160.000","3":"36.34","4":"11.12","5":"18.17","6":"44.03"},{"1":"9","2":"90.090","3":"25.01","4":"9.62","5":"10.94","6":"36.02"},{"1":"18","2":"70.000","3":"18.00","4":"8.67","5":"9.00","6":"38.89"},{"1":"6","2":"158.000","3":"43.70","4":"11.25","5":"18.00","6":"36.16"},{"1":"21","2":"73.941","3":"27.44","4":"8.80","5":"11.75","6":"26.95"},{"1":"10","2":"138.000","3":"31.14","4":"10.20","5":"15.57","6":"44.32"},{"1":"22","2":"73.941","3":"27.44","4":"8.80","5":"11.77","6":"30.94"},{"1":"11","2":"138.000","3":"31.14","4":"10.20","5":"15.57","6":"44.32"},{"1":"4","2":"220.000","3":"54.00","4":"11.82","5":"27.00","6":"40.74"},{"1":"12","2":"90.090","3":"25.01","4":"9.62","5":"10.50","6":"36.02"},{"1":"16","2":"78.491","3":"24.35","4":"9.15","5":"10.00","6":"32.23"},{"1":"10","2":"90.090","3":"25.01","4":"9.62","5":"10.50","6":"36.02"},{"1":"25","2":"73.192","3":"28.52","4":"8.80","5":"11.38","6":"25.66"},{"1":"17","2":"70.000","3":"20.76","4":"8.67","5":"9.02","6":"33.72"},{"1":"15","2":"78.491","3":"24.35","4":"9.15","5":"10.00","6":"32.23"},{"1":"14","2":"138.000","3":"31.14","4":"10.20","5":"15.57","6":"44.32"},{"1":"21","2":"10.000","3":"2.08","4":"4.40","5":"1.04","6":"48.08"},{"1":"27","2":"10.000","3":"2.08","4":"4.40","5":"1.04","6":"48.08"},{"1":"24","2":"10.000","3":"2.08","4":"4.40","5":"1.04","6":"48.08"},{"1":"19","2":"16.800","3":"2.96","4":"5.14","5":"1.48","6":"56.76"},{"1":"13","2":"25.000","3":"3.82","4":"5.97","5":"1.94","6":"65.45"},{"1":"12","2":"25.000","3":"3.88","4":"5.97","5":"1.94","6":"64.43"},{"1":"19","2":"16.800","3":"2.96","4":"5.14","5":"1.48","6":"56.76"},{"1":"22","2":"3.341","3":"0.66","4":"2.80","5":"0.33","6":"50.62"},{"1":"21","2":"19.093","3":"8.00","4":"5.37","5":"4.00","6":"23.87"},{"1":"12","2":"42.000","3":"14.80","4":"7.13","5":"7.40","6":"28.38"},{"1":"24","2":"40.053","3":"12.87","4":"5.79","5":"7.76","6":"31.12"},{"1":"22","2":"3.341","3":"0.66","4":"2.79","5":"0.33","6":"50.62"},{"1":"14","2":"76.800","3":"19.60","4":"8.79","5":"9.67","6":"39.18"},{"1":"25","2":"5.350","3":"1.58","4":"4.40","5":"0.74","6":"33.86"},{"1":"27","2":"5.350","3":"1.67","4":"4.40","5":"0.74","6":"32.04"},{"1":"23","2":"14.745","3":"3.08","4":"6.17","5":"1.56","6":"47.87"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Now we are all set up to implement permutation importance in python and in R</p>
<p><br><br><br> (<strong><em>Click tabs below to change language!</em></strong>)</p>
<div id="in-python" class="section level3">
<h3>In Python</h3>
<p>First, lets set up three models to test: A linear model, a neural network, and a random forest:</p>
<div class="blue">
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb3-2"><a href="#cb3-2"></a>knn <span class="op">=</span> KNeighborsRegressor(<span class="dv">13</span>) </span>
<span id="cb3-3"><a href="#cb3-3"></a>rf <span class="op">=</span> RandomForestRegressor(n_estimators <span class="op">=</span> <span class="dv">100</span>)</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a>models <span class="op">=</span> [lm, knn, rf]</span>
<span id="cb3-6"><a href="#cb3-6"></a></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="cf">for</span> m <span class="kw">in</span> models:</span>
<span id="cb3-8"><a href="#cb3-8"></a>  m.fit(X_train, y_train)</span></code></pre></div>
<pre><code>#&gt; LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)
#&gt; KNeighborsRegressor(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
#&gt;                     metric_params=None, n_jobs=None, n_neighbors=13, p=2,
#&gt;                     weights=&#39;uniform&#39;)
#&gt; RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None,
#&gt;                       max_features=&#39;auto&#39;, max_leaf_nodes=None,
#&gt;                       min_impurity_decrease=0.0, min_impurity_split=None,
#&gt;                       min_samples_leaf=1, min_samples_split=2,
#&gt;                       min_weight_fraction_leaf=0.0, n_estimators=100,
#&gt;                       n_jobs=None, oob_score=False, random_state=None,
#&gt;                       verbose=0, warm_start=False)</code></pre>
<p>Lets check out as a baseline truth which features are important in the random forest, and the strength of the predictors in our linear regression:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>pprint(<span class="bu">dict</span>(<span class="bu">zip</span>(X_train.columns, rf.feature_importances_)))</span></code></pre></div>
<pre><code>#&gt; {&#39;Age&#39;: 0.013535894172965822,
#&gt;  &#39;Tonnage&#39;: 0.3747625842890873,
#&gt;  &#39;cabins&#39;: 0.45156928647564787,
#&gt;  &#39;length&#39;: 0.06046162173630532,
#&gt;  &#39;passenger_density&#39;: 0.012523380644943075,
#&gt;  &#39;passengers&#39;: 0.08714723268105071}</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>pprint({X_train.columns[i]: lm.coef_[:,i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(lm.coef_.shape[<span class="dv">1</span>])})</span></code></pre></div>
<pre><code>#&gt; {&#39;Age&#39;: array([-0.01321228]),
#&gt;  &#39;Tonnage&#39;: array([0.00316145]),
#&gt;  &#39;cabins&#39;: array([0.79858261]),
#&gt;  &#39;length&#39;: array([0.39111394]),
#&gt;  &#39;passenger_density&#39;: array([0.01037499]),
#&gt;  &#39;passengers&#39;: array([-0.1045376])}</code></pre>
<p>Looks like tonnage and cabins are the most important variables for the random forest, and cabins, length, and passengers for linear regression.</p>
<p>Next, lets create a function for permutation importance! This should be pretty easy to do, I have added in a few extra components for thoroughness, but the code is not hard:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">def</span> permutation_importance(model, x, y, loss, base <span class="op">=</span> <span class="va">False</span>, x_train <span class="op">=</span> <span class="va">None</span>, y_train <span class="op">=</span> <span class="va">None</span>, kind <span class="op">=</span> <span class="st">&quot;prop&quot;</span>, n_rounds <span class="op">=</span> <span class="dv">5</span>):</span>
<span id="cb9-2"><a href="#cb9-2"></a>    explan <span class="op">=</span> x.columns</span>
<span id="cb9-3"><a href="#cb9-3"></a>    baseline <span class="op">=</span> loss(y, model.predict(x))</span>
<span id="cb9-4"><a href="#cb9-4"></a>    res <span class="op">=</span> {k:[] <span class="cf">for</span> k <span class="kw">in</span> explan}</span>
<span id="cb9-5"><a href="#cb9-5"></a>    <span class="cf">if</span> (base <span class="kw">is</span> <span class="va">True</span>):</span>
<span id="cb9-6"><a href="#cb9-6"></a>        res[<span class="st">&quot;baseline&quot;</span>] <span class="op">=</span> []</span>
<span id="cb9-7"><a href="#cb9-7"></a>    <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_rounds):</span>
<span id="cb9-8"><a href="#cb9-8"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(explan)):</span>
<span id="cb9-9"><a href="#cb9-9"></a>            col <span class="op">=</span> explan[i]</span>
<span id="cb9-10"><a href="#cb9-10"></a>            x_temp <span class="op">=</span> x.copy()</span>
<span id="cb9-11"><a href="#cb9-11"></a>            x_temp[col] <span class="op">=</span>  np.random.permutation(x_temp[col])</span>
<span id="cb9-12"><a href="#cb9-12"></a>            <span class="cf">if</span> (kind <span class="kw">is</span> <span class="kw">not</span> <span class="st">&quot;prop&quot;</span>):</span>
<span id="cb9-13"><a href="#cb9-13"></a>                res[col].append(loss(y, model.predict(x_temp)) <span class="op">-</span>  baseline)</span>
<span id="cb9-14"><a href="#cb9-14"></a>            <span class="cf">else</span>:</span>
<span id="cb9-15"><a href="#cb9-15"></a>                res[col].append(loss(y, model.predict(x_temp)) <span class="op">/</span>  baseline)</span>
<span id="cb9-16"><a href="#cb9-16"></a>        <span class="cf">if</span> (base <span class="kw">is</span> <span class="va">True</span>):</span>
<span id="cb9-17"><a href="#cb9-17"></a>            x_temp <span class="op">=</span> x.copy()</span>
<span id="cb9-18"><a href="#cb9-18"></a>            x_train2 <span class="op">=</span> x_train.copy()</span>
<span id="cb9-19"><a href="#cb9-19"></a>            x_temp[<span class="st">&quot;baseline&quot;</span>] <span class="op">=</span> np.clip(np.random.normal(size <span class="op">=</span> <span class="bu">len</span>(x_temp)), <span class="fl">-1.</span>, <span class="fl">1.</span>)</span>
<span id="cb9-20"><a href="#cb9-20"></a>            x_train2[<span class="st">&quot;baseline&quot;</span>] <span class="op">=</span> np.clip(np.random.normal(size <span class="op">=</span> <span class="bu">len</span>(x_train2)), <span class="fl">-1.</span>, <span class="fl">1.</span>)</span>
<span id="cb9-21"><a href="#cb9-21"></a>            mod2 <span class="op">=</span> <span class="bu">type</span>(model)()</span>
<span id="cb9-22"><a href="#cb9-22"></a>            mod2.fit(x_train2, y_train)</span>
<span id="cb9-23"><a href="#cb9-23"></a>            <span class="cf">if</span> (kind <span class="kw">is</span> <span class="kw">not</span> <span class="st">&quot;prop&quot;</span>):</span>
<span id="cb9-24"><a href="#cb9-24"></a>                res[<span class="st">&quot;baseline&quot;</span>].append(loss(y, mod2.predict(x_temp)) <span class="op">-</span>  baseline)</span>
<span id="cb9-25"><a href="#cb9-25"></a>            <span class="cf">else</span>:</span>
<span id="cb9-26"><a href="#cb9-26"></a>                res[<span class="st">&quot;baseline&quot;</span>].append(loss(y, mod2.predict(x_temp)) <span class="op">/</span>  baseline)</span>
<span id="cb9-27"><a href="#cb9-27"></a>    <span class="cf">return</span>(pd.DataFrame.from_dict(res))</span></code></pre></div>
<p>Lets also define a helper function to help us do this for all our models, and then go ahead and calculate the importances!</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># convert object name to string!</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="kw">def</span> get_name(obj):</span>
<span id="cb10-3"><a href="#cb10-3"></a>    name <span class="op">=</span>[x <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">globals</span>() <span class="cf">if</span> <span class="bu">globals</span>()[x] <span class="kw">is</span> obj][<span class="dv">0</span>]</span>
<span id="cb10-4"><a href="#cb10-4"></a>    <span class="cf">return</span>(name)</span>
<span id="cb10-5"><a href="#cb10-5"></a></span>
<span id="cb10-6"><a href="#cb10-6"></a>imps <span class="op">=</span> {}</span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="cf">for</span> m <span class="kw">in</span> models:</span>
<span id="cb10-8"><a href="#cb10-8"></a>    imps[get_name(m)] <span class="op">=</span> permutation_importance(m, X_test, y_test, loss_mse, <span class="va">True</span>,  X_train, y_train, n_rounds <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb10-9"><a href="#cb10-9"></a>pprint(imps)</span></code></pre></div>
<pre><code>#&gt; {&#39;knn&#39;:         Age    Tonnage  passengers    length    cabins  passenger_density  baseline
#&gt; 0  0.925346  30.092454    1.265504  1.005519  0.990236           1.609531  1.310816
#&gt; 1  0.998424  18.854886    1.098596  0.991386  1.033093           1.399524  1.309873
#&gt; 2  0.924789  19.048532    1.136701  1.014133  1.011349           1.303790  1.307799
#&gt; 3  1.029874  23.821313    1.055337  1.005181  0.960479           1.511165  1.325939
#&gt; 4  0.925670  19.511069    1.205205  1.013730  0.993391           1.396396  1.313307,
#&gt;  &#39;lm&#39;:         Age   Tonnage  passengers    length     cabins  passenger_density  baseline
#&gt; 0  1.027923  1.150192    7.316054  4.261241  66.162984           1.014724  1.003418
#&gt; 1  1.032696  1.028111    8.185104  5.974532  72.536001           1.051532  1.089962
#&gt; 2  1.040655  0.991113    8.187338  5.321326  82.918403           1.143379  1.076748
#&gt; 3  1.107886  1.105453    7.230693  4.273769  66.157890           1.014087  1.002733
#&gt; 4  1.015266  1.067878    8.607417  4.721843  65.515803           1.076129  1.054455,
#&gt;  &#39;rf&#39;:         Age    Tonnage  passengers    length     cabins  passenger_density  baseline
#&gt; 0  1.186356  15.370202    2.569056  1.563294  22.342656           1.691316  1.683120
#&gt; 1  1.222609  16.300105    2.817115  2.398477  26.987853           1.787926  1.370588
#&gt; 2  1.051913   9.894557    2.784793  2.892679  14.818642           1.718829  1.287242
#&gt; 3  1.230700  14.602542    2.469666  2.245964  13.494130           1.618582  2.240007
#&gt; 4  1.160245  14.985467    2.957951  1.659667  19.721395           1.645616  1.310004}</code></pre>
<p>This output is a bit hard to read, so lets go ahead and write a helper function which averages the importances, and plots them nicely!</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>plt.style.use(<span class="st">&quot;ggplot&quot;</span>)</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="kw">def</span> plot_perm_imp(df, ax <span class="op">=</span> <span class="va">None</span>, color <span class="op">=</span> <span class="st">&#39;blue&#39;</span>):</span>
<span id="cb12-3"><a href="#cb12-3"></a>    <span class="co"># mean the columns and put it back into a data frame</span></span>
<span id="cb12-4"><a href="#cb12-4"></a>    df1 <span class="op">=</span> (df.<span class="bu">apply</span>(stats.mean, <span class="dv">0</span>, result_type <span class="op">=</span> <span class="st">&quot;broadcast&quot;</span>)).drop(df.index[<span class="dv">1</span>:])</span>
<span id="cb12-5"><a href="#cb12-5"></a>    <span class="co"># create a new data frame excluding baseline, so we can do something special with it</span></span>
<span id="cb12-6"><a href="#cb12-6"></a>    df_temp <span class="op">=</span> df1.loc[:, df.columns <span class="op">!=</span> <span class="st">&#39;baseline&#39;</span>]</span>
<span id="cb12-7"><a href="#cb12-7"></a>    <span class="co"># melt and sort it for plotting</span></span>
<span id="cb12-8"><a href="#cb12-8"></a>    df2 <span class="op">=</span> df_temp.melt(var_name <span class="op">=</span> <span class="st">&#39;variable&#39;</span>, value_name <span class="op">=</span> <span class="st">&#39;importance&#39;</span>)</span>
<span id="cb12-9"><a href="#cb12-9"></a>    df2 <span class="op">=</span> df2.sort_values(by <span class="op">=</span> <span class="st">&quot;importance&quot;</span>)</span>
<span id="cb12-10"><a href="#cb12-10"></a>    <span class="co"># plot it nicely</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>    df2.plot(kind <span class="op">=</span> <span class="st">&#39;barh&#39;</span>, x <span class="op">=</span> <span class="st">&#39;variable&#39;</span>, y <span class="op">=</span> <span class="st">&#39;importance&#39;</span>, width <span class="op">=</span> <span class="fl">0.8</span>, ax <span class="op">=</span> ax, color <span class="op">=</span> color)</span>
<span id="cb12-12"><a href="#cb12-12"></a>    <span class="co"># draw a bar and an arrow to baseline</span></span>
<span id="cb12-13"><a href="#cb12-13"></a>    <span class="cf">for</span> n <span class="kw">in</span> df1.columns:</span>
<span id="cb12-14"><a href="#cb12-14"></a>        <span class="cf">if</span> n <span class="kw">is</span> <span class="st">&quot;baseline&quot;</span>:</span>
<span id="cb12-15"><a href="#cb12-15"></a>            plt.axvline(x <span class="op">=</span> df1[n][<span class="dv">0</span>])</span>
<span id="cb12-16"><a href="#cb12-16"></a>            plt.annotate(<span class="st">&#39;baseline&#39;</span>,</span>
<span id="cb12-17"><a href="#cb12-17"></a>                         xy <span class="op">=</span> (df1[n][<span class="dv">0</span>], <span class="dv">1</span>),</span>
<span id="cb12-18"><a href="#cb12-18"></a>                         xytext <span class="op">=</span> (df1[n][<span class="dv">0</span>] <span class="op">+</span> <span class="fl">0.4</span>, <span class="dv">3</span>),</span>
<span id="cb12-19"><a href="#cb12-19"></a>                         arrowprops <span class="op">=</span> <span class="bu">dict</span>(facecolor <span class="op">=</span> <span class="st">&#39;black&#39;</span>,</span>
<span id="cb12-20"><a href="#cb12-20"></a>                                           shrink <span class="op">=</span> <span class="fl">0.05</span>),</span>
<span id="cb12-21"><a href="#cb12-21"></a>                         bbox <span class="op">=</span> <span class="bu">dict</span>(boxstyle <span class="op">=</span> <span class="st">&quot;square&quot;</span>, fc <span class="op">=</span> (<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)))</span>
<span id="cb12-22"><a href="#cb12-22"></a></span>
<span id="cb12-23"><a href="#cb12-23"></a></span>
<span id="cb12-24"><a href="#cb12-24"></a><span class="co"># plot the importance dict!</span></span>
<span id="cb12-25"><a href="#cb12-25"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>))</span>
<span id="cb12-26"><a href="#cb12-26"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(imps.keys())):</span>
<span id="cb12-27"><a href="#cb12-27"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="bu">len</span>(<span class="bu">list</span>(imps.keys())),<span class="dv">1</span>, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb12-28"><a href="#cb12-28"></a>    c <span class="op">=</span> sns.color_palette(<span class="st">&quot;hls&quot;</span>, i<span class="op">+</span><span class="dv">1</span>)[i]</span>
<span id="cb12-29"><a href="#cb12-29"></a>    plot_perm_imp(imps[<span class="bu">list</span>(imps.keys())[i]], ax <span class="op">=</span> ax, color <span class="op">=</span> c)</span>
<span id="cb12-30"><a href="#cb12-30"></a>    ax.set_title(<span class="bu">list</span>(imps.keys())[i])</span>
<span id="cb12-31"><a href="#cb12-31"></a>    <span class="cf">for</span> tick <span class="kw">in</span> ax.yaxis.get_major_ticks():</span>
<span id="cb12-32"><a href="#cb12-32"></a>      tick.label.set_fontsize(<span class="st">&quot;x-small&quot;</span>)</span>
<span id="cb12-33"><a href="#cb12-33"></a>      tick.label.set_rotation(<span class="dv">45</span>)</span>
<span id="cb12-34"><a href="#cb12-34"></a>plt.show()</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="iml_files/figure-html/unnamed-chunk-7-1.svg" alt="Permutation Importances for linear regression, knn, and a random forest"  />
<p class="caption">
Permutation Importances for linear regression, knn, and a random forest
</p>
</div>
<p>Looks like it worked alright!! Especially for random forest, we nicely identified the two most important features, however with little granularity. With our linear model, it is a little less clear, but we do see that the same three most important features carry through! However, we have no real granularity or idea the scale of the effect of the variable. We also carry a risk with permutation importance and correlated variables in general: we are using often unrealistic observations. For example, we may be looking a tiny boats (in length) which weigh the same as the largest boats (they would sink!!) with our permutations. This leads to often unreliable output with many permutation based tools, which we will also explore in this post.</p>
</div>
</div>
<div id="in-r" class="section level3">
<h3>In R</h3>
First, lets set up three models to test: A linear model, a knn model, and a random forest:
<div class="blue">
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="kw">library</span>(randomForest)</span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="kw">library</span>(kknn)</span>
<span id="cb13-3"><a href="#cb13-3"></a></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="co"># set up models with parameters</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>rf &lt;-<span class="st"> </span><span class="cf">function</span>(df) {</span>
<span id="cb13-6"><a href="#cb13-6"></a>  <span class="kw">return</span>(<span class="kw">randomForest</span>(crew <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> df, <span class="dt">ntree =</span> <span class="dv">100</span>))</span>
<span id="cb13-7"><a href="#cb13-7"></a>}</span>
<span id="cb13-8"><a href="#cb13-8"></a></span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="co"># knn in R is annoying so we will need to a consistent api ourselves:</span></span>
<span id="cb13-10"><a href="#cb13-10"></a></span>
<span id="cb13-11"><a href="#cb13-11"></a>knn &lt;-<span class="st"> </span><span class="cf">function</span>(df) {</span>
<span id="cb13-12"><a href="#cb13-12"></a>  res &lt;-<span class="st"> </span><span class="kw">list</span>()</span>
<span id="cb13-13"><a href="#cb13-13"></a>  res<span class="op">$</span>train &lt;-<span class="st"> </span>df</span>
<span id="cb13-14"><a href="#cb13-14"></a>  res<span class="op">$</span>k &lt;-<span class="st"> </span><span class="dv">13</span></span>
<span id="cb13-15"><a href="#cb13-15"></a>  res &lt;-<span class="st"> </span><span class="kw">structure</span>(res, <span class="dt">class =</span> <span class="st">&quot;knn&quot;</span>)</span>
<span id="cb13-16"><a href="#cb13-16"></a>}</span>
<span id="cb13-17"><a href="#cb13-17"></a></span>
<span id="cb13-18"><a href="#cb13-18"></a>predict.knn &lt;-<span class="st"> </span><span class="cf">function</span>(obj, newdata) {</span>
<span id="cb13-19"><a href="#cb13-19"></a>  out &lt;-<span class="st">  </span><span class="kw">kknn</span>(crew <span class="op">~</span><span class="st"> </span>., <span class="dt">train =</span> obj<span class="op">$</span>train, <span class="dt">test =</span> newdata, <span class="dt">k =</span> <span class="dv">13</span>)</span>
<span id="cb13-20"><a href="#cb13-20"></a>  <span class="kw">return</span>(<span class="kw">as.numeric</span>(out<span class="op">$</span>fitted.values))</span>
<span id="cb13-21"><a href="#cb13-21"></a>}</span>
<span id="cb13-22"><a href="#cb13-22"></a></span>
<span id="cb13-23"><a href="#cb13-23"></a>linear_model &lt;-<span class="st"> </span><span class="cf">function</span>(df) {</span>
<span id="cb13-24"><a href="#cb13-24"></a>  <span class="kw">lm</span>(crew <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> df)</span>
<span id="cb13-25"><a href="#cb13-25"></a>}</span>
<span id="cb13-26"><a href="#cb13-26"></a></span>
<span id="cb13-27"><a href="#cb13-27"></a>models &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;lm&quot;</span> =<span class="st"> </span>linear_model,<span class="st">&quot;knn&quot;</span>=<span class="st"> </span>knn,<span class="st">&quot;rf&quot;</span>=<span class="st"> </span>rf)</span>
<span id="cb13-28"><a href="#cb13-28"></a></span>
<span id="cb13-29"><a href="#cb13-29"></a>t_train &lt;-<span class="st"> </span><span class="kw">cbind</span>(X_train, y_train)</span>
<span id="cb13-30"><a href="#cb13-30"></a></span>
<span id="cb13-31"><a href="#cb13-31"></a>trained_models &lt;-<span class="st"> </span><span class="kw">lapply</span>(models, <span class="cf">function</span>(f) <span class="kw">f</span>(t_train))</span></code></pre></div>
<p>Lets check out as a baseline truth which features are important in the random forest, and the strength of the predictors in our linear regression:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a>pander<span class="op">::</span><span class="kw">pander</span>(<span class="kw">summary</span>(trained_models[[<span class="st">&quot;lm&quot;</span>]]))</span></code></pre></div>
<table style="width:96%;">
<colgroup>
<col width="33%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">-0.9349</td>
<td align="center">1.878</td>
<td align="center">-0.4977</td>
<td align="center">0.6197</td>
</tr>
<tr class="even">
<td align="center"><strong>Age</strong></td>
<td align="center">-0.01321</td>
<td align="center">0.02083</td>
<td align="center">-0.6343</td>
<td align="center">0.5272</td>
</tr>
<tr class="odd">
<td align="center"><strong>Tonnage</strong></td>
<td align="center">0.003161</td>
<td align="center">0.01692</td>
<td align="center">0.1868</td>
<td align="center">0.8522</td>
</tr>
<tr class="even">
<td align="center"><strong>passengers</strong></td>
<td align="center">-0.1045</td>
<td align="center">0.06831</td>
<td align="center">-1.53</td>
<td align="center">0.1288</td>
</tr>
<tr class="odd">
<td align="center"><strong>length</strong></td>
<td align="center">0.3911</td>
<td align="center">0.1624</td>
<td align="center">2.409</td>
<td align="center">0.01765</td>
</tr>
<tr class="even">
<td align="center"><strong>cabins</strong></td>
<td align="center">0.7986</td>
<td align="center">0.1058</td>
<td align="center">7.545</td>
<td align="center">1.34e-11</td>
</tr>
<tr class="odd">
<td align="center"><strong>passenger_density</strong></td>
<td align="center">0.01037</td>
<td align="center">0.02618</td>
<td align="center">0.3963</td>
<td align="center">0.6927</td>
</tr>
</tbody>
</table>
<table style="width:86%;">
<caption>Fitting linear model: crew ~ .</caption>
<colgroup>
<col width="20%" />
<col width="30%" />
<col width="11%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">118</td>
<td align="center">1.089</td>
<td align="center">0.906</td>
<td align="center">0.9009</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>pander<span class="op">::</span><span class="kw">pander</span>(<span class="kw">importance</span>(trained_models<span class="op">$</span>rf))</span></code></pre></div>
<table style="width:56%;">
<colgroup>
<col width="33%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">IncNodePurity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Age</strong></td>
<td align="center">59.01</td>
</tr>
<tr class="even">
<td align="center"><strong>Tonnage</strong></td>
<td align="center">361.2</td>
</tr>
<tr class="odd">
<td align="center"><strong>passengers</strong></td>
<td align="center">222.1</td>
</tr>
<tr class="even">
<td align="center"><strong>length</strong></td>
<td align="center">257.9</td>
</tr>
<tr class="odd">
<td align="center"><strong>cabins</strong></td>
<td align="center">421.3</td>
</tr>
<tr class="even">
<td align="center"><strong>passenger_density</strong></td>
<td align="center">28.63</td>
</tr>
</tbody>
</table>
<p>Looks like tonnage and cabins are the most important variables for the random forest, and cabins, length, and passengers for linear regression.</p>
<p>Next, lets create a function for permutation importance! This should be pretty easy to do, I have added in a few extra components for thoroughness, but the code is not hard:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># loss function</span></span>
<span id="cb16-2"><a href="#cb16-2"></a>loss_mse &lt;-<span class="st"> </span><span class="cf">function</span>(truth, preds) {</span>
<span id="cb16-3"><a href="#cb16-3"></a>  error &lt;-<span class="st"> </span>truth <span class="op">-</span><span class="st"> </span>preds</span>
<span id="cb16-4"><a href="#cb16-4"></a>  square_error &lt;-<span class="st"> </span>error<span class="op">^</span><span class="dv">2</span></span>
<span id="cb16-5"><a href="#cb16-5"></a>  <span class="kw">return</span>(<span class="kw">mean</span>(square_error))</span>
<span id="cb16-6"><a href="#cb16-6"></a>}</span>
<span id="cb16-7"><a href="#cb16-7"></a></span>
<span id="cb16-8"><a href="#cb16-8"></a><span class="co"># get baseline loss</span></span>
<span id="cb16-9"><a href="#cb16-9"></a>get_loss &lt;-<span class="st"> </span><span class="cf">function</span>(model, x, y, loss) {</span>
<span id="cb16-10"><a href="#cb16-10"></a>  <span class="kw">loss</span>(y, <span class="kw">predict</span>(model, x))</span>
<span id="cb16-11"><a href="#cb16-11"></a>}</span>
<span id="cb16-12"><a href="#cb16-12"></a></span>
<span id="cb16-13"><a href="#cb16-13"></a></span>
<span id="cb16-14"><a href="#cb16-14"></a><span class="co"># permute a single column</span></span>
<span id="cb16-15"><a href="#cb16-15"></a>permute_column &lt;-<span class="st"> </span><span class="cf">function</span>(df, col) {</span>
<span id="cb16-16"><a href="#cb16-16"></a>  df[[col]] &lt;-<span class="st"> </span>df[[col]][<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(df))]</span>
<span id="cb16-17"><a href="#cb16-17"></a>  <span class="kw">return</span>(df)</span>
<span id="cb16-18"><a href="#cb16-18"></a>}</span>
<span id="cb16-19"><a href="#cb16-19"></a></span>
<span id="cb16-20"><a href="#cb16-20"></a></span>
<span id="cb16-21"><a href="#cb16-21"></a>permutation_importance &lt;-<span class="st"> </span><span class="cf">function</span>(model, x, y, loss, <span class="dt">x_train =</span> <span class="ot">NA</span>, <span class="dt">y_train =</span> <span class="ot">NA</span>, <span class="dt">n_rounds =</span> <span class="dv">5</span>) {</span>
<span id="cb16-22"><a href="#cb16-22"></a>  baseline &lt;-<span class="st"> </span><span class="kw">get_loss</span>(model, x, y[[<span class="dv">1</span>]], loss)</span>
<span id="cb16-23"><a href="#cb16-23"></a>  explan &lt;-<span class="st"> </span><span class="kw">names</span>(x)</span>
<span id="cb16-24"><a href="#cb16-24"></a>  single_round_imp &lt;-<span class="st"> </span><span class="cf">function</span>(df) {</span>
<span id="cb16-25"><a href="#cb16-25"></a>    dfs &lt;-<span class="st"> </span><span class="kw">lapply</span>(explan, <span class="cf">function</span>(i) <span class="kw">permute_column</span>(x, i))</span>
<span id="cb16-26"><a href="#cb16-26"></a>    <span class="kw">return</span>(<span class="kw">vapply</span>(dfs, <span class="cf">function</span>(i) <span class="kw">get_loss</span>(model, i, y[[<span class="dv">1</span>]], loss), <span class="kw">numeric</span>(<span class="dv">1</span>)))</span>
<span id="cb16-27"><a href="#cb16-27"></a>  }</span>
<span id="cb16-28"><a href="#cb16-28"></a>  res &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>n_rounds, <span class="cf">function</span>(x) <span class="kw">single_round_imp</span>(df))</span>
<span id="cb16-29"><a href="#cb16-29"></a>  res &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">do.call</span>(rbind, res))</span>
<span id="cb16-30"><a href="#cb16-30"></a>  res &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">lapply</span>(res, <span class="cf">function</span>(x) x<span class="op">/</span>baseline))</span>
<span id="cb16-31"><a href="#cb16-31"></a>  <span class="kw">names</span>(res) &lt;-<span class="st"> </span><span class="kw">names</span>(x)</span>
<span id="cb16-32"><a href="#cb16-32"></a>  <span class="kw">return</span>(<span class="kw">as.data.frame</span>(<span class="kw">lapply</span>(res, mean)))</span>
<span id="cb16-33"><a href="#cb16-33"></a>}</span>
<span id="cb16-34"><a href="#cb16-34"></a></span>
<span id="cb16-35"><a href="#cb16-35"></a><span class="co"># we can do this in a second because R is speedy </span></span>
<span id="cb16-36"><a href="#cb16-36"></a>perm_vips &lt;-<span class="st"> </span><span class="kw">lapply</span>(trained_models, <span class="cf">function</span>(x) <span class="kw">permutation_importance</span>(x, X_test, y_test, loss_mse, <span class="dt">n_rounds =</span> <span class="dv">30</span>))</span>
<span id="cb16-37"><a href="#cb16-37"></a>pander<span class="op">::</span><span class="kw">pander</span>(perm_vips)</span></code></pre></div>
<ul>
<li><p><strong>lm</strong>:</p>
<table style="width:96%;">
<colgroup>
<col width="11%" />
<col width="13%" />
<col width="18%" />
<col width="12%" />
<col width="12%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Age</th>
<th align="center">Tonnage</th>
<th align="center">passengers</th>
<th align="center">length</th>
<th align="center">cabins</th>
<th align="center">passenger_density</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1.029</td>
<td align="center">1.056</td>
<td align="center">7.572</td>
<td align="center">4.568</td>
<td align="center">72.37</td>
<td align="center">1.089</td>
</tr>
</tbody>
</table></li>
<li><p><strong>knn</strong>:</p>
<table style="width:96%;">
<colgroup>
<col width="11%" />
<col width="13%" />
<col width="18%" />
<col width="12%" />
<col width="12%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Age</th>
<th align="center">Tonnage</th>
<th align="center">passengers</th>
<th align="center">length</th>
<th align="center">cabins</th>
<th align="center">passenger_density</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1.262</td>
<td align="center">1.773</td>
<td align="center">2.049</td>
<td align="center">3.093</td>
<td align="center">2.738</td>
<td align="center">1.234</td>
</tr>
</tbody>
</table></li>
<li><p><strong>rf</strong>:</p>
<table style="width:96%;">
<colgroup>
<col width="11%" />
<col width="13%" />
<col width="18%" />
<col width="12%" />
<col width="12%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Age</th>
<th align="center">Tonnage</th>
<th align="center">passengers</th>
<th align="center">length</th>
<th align="center">cabins</th>
<th align="center">passenger_density</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1.132</td>
<td align="center">7.509</td>
<td align="center">3.906</td>
<td align="center">4.065</td>
<td align="center">9.586</td>
<td align="center">1.429</td>
</tr>
</tbody>
</table></li>
</ul>
<!-- end of list -->
<p>So, it looks like our importances are quite similar to the importances calculated by the models explicitly, so not a bad job! Lets go ahead and produce a nice plot!</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb17-2"><a href="#cb17-2"></a>vip_flat &lt;-<span class="st"> </span><span class="kw">lapply</span>(perm_vips, gather)</span>
<span id="cb17-3"><a href="#cb17-3"></a></span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="co"># Finally an appropriate use of superassignment!!</span></span>
<span id="cb17-5"><a href="#cb17-5"></a><span class="kw">lapply</span>(<span class="kw">names</span>(vip_flat), <span class="cf">function</span>(x) {</span>
<span id="cb17-6"><a href="#cb17-6"></a>  vip_flat[[x]][[<span class="st">&quot;model&quot;</span>]] &lt;&lt;-<span class="st"> </span>x</span>
<span id="cb17-7"><a href="#cb17-7"></a>})  <span class="op">%&gt;%</span><span class="st"> </span>invisible</span>
<span id="cb17-8"><a href="#cb17-8"></a></span>
<span id="cb17-9"><a href="#cb17-9"></a><span class="kw">do.call</span>( rbind, vip_flat) <span class="op">%&gt;%</span><span class="st"> </span>as.data.frame <span class="op">%&gt;%</span></span>
<span id="cb17-10"><a href="#cb17-10"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> key, <span class="dt">y =</span> value, <span class="dt">fill =</span> model)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="op">+</span></span>
<span id="cb17-11"><a href="#cb17-11"></a><span class="st">  </span><span class="kw">facet_grid</span>(model <span class="op">~</span><span class="st"> </span>., <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">coord_flip</span>() <span class="op">+</span><span class="st"> </span>ggthemes<span class="op">::</span><span class="kw">theme_fivethirtyeight</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb17-12"><a href="#cb17-12"></a><span class="st">  </span>ggthemes<span class="op">::</span><span class="kw">scale_fill_fivethirtyeight</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Permutation Importance&quot;</span>)</span></code></pre></div>
<p><img src="iml_files/figure-html/unnamed-chunk-11-1.svg" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Looks like it worked alright!! Especially for random forest, we nicely identified the two most important features, however with little granularity. With our linear model, it is a little less clear, but we do see that the same three most important features carry through! However, we have no real granularity or idea the scale of the effect of the variable. We also carry a risk with permutation importance and correlated variables in general: we are using often unrealistic observations. For example, we may be looking a tiny boats (in length) which weigh the same as the largest boats (they would sink!!) with our permutations. This leads to often unreliable output with many permutation based tools, which we will also explore in this post.</p>
</div>
</div>
</div>
<div id="partial-dependence" class="section level1">
<h1>Partial Dependence</h1>
<p>Let’s try and think of a annother way to quantify the effect of a variable. We previously answered the question: “How much worse will my prediction be, given a feature has been replaced with noise?”. Now, lets ask a new question: &gt; What will my prediction be, in general, when a feature is at a specific value?</p>
<p>This is Partial Dependence in a nutshell. Stemming off from this, we can make the claim:</p>
<blockquote>
<p>If a feature is more important, the prediction will vary more with that feature than others.</p>
</blockquote>
<p>This is exactly the claim made (and supported) in <a href="https://arxiv.org/abs/1805.04755">this excellent paper</a>. Thus in this section, we will calculate two things: Partial Dependence, and Partial Dependence Importance. Partial Dependence, at a high level, is calculated like this:</p>
<ol style="list-style-type: decimal">
<li>Take your original data, copy it</li>
<li>Replace the feature of interest with the value of the first observation of the feature of interest.</li>
<li>Calculate new prediction vector</li>
<li>Average that</li>
<li>Repeat for all observations</li>
</ol>
<p>And then we can just take more or less the standard deviation of these curves to estimate importance (its a bit different for categorical variables, see the paper).</p>
<div id="formalization-1" class="section level2">
<h2>Formalization</h2>
<p>Consider again <span class="math inline">\(x = (x_s, x_c)\)</span></p>
<p>We can define the partial dependence as the expected value of our prediction function, <span class="math inline">\(\hat f\)</span>, given <span class="math inline">\(x_c\)</span>:</p>
<p><span class="math display">\[\mathrm{PDP}(z_s) = E \left[ \hat f (x_s, x_c) \mid x_c  \right]\]</span></p>
<p><span class="math display">\[ = \int \hat f (x_s, x_c) \mathbb{P}_c(x_c) dx_c\]</span> Where <span class="math inline">\(\mathbb{P}_c\)</span> is the marginal distribution of <span class="math inline">\(x_c\)</span>, <span class="math inline">\(\int p(x)dx_s\)</span>.</p>
<p>We can then formulate this for a finite set of <span class="math inline">\(n\)</span> features using the Monte Carlo method:</p>
<p><span class="math display">\[\widetilde {\mathrm{PDP}} (x_s) = \frac{1}{n} \sum_{i=1}^{i=k} \hat f (x_s, x_c^{(i)})\]</span></p>
<p>where <span class="math inline">\(x_c^{(i)}\)</span> is a distinct observation(row) of <span class="math inline">\(x_c\)</span></p>
</div>
<div id="code-implementation-1" class="section level2 tabset tabset-pills">
<h2>Code Implementation</h2>
<div id="in-python-1" class="section level3">
<h3>In Python</h3>
<p>First, lets define a function to calculate the partial dependence of a prediction on a single variable:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="kw">def</span> pdp_var(model, x, var):</span>
<span id="cb18-2"><a href="#cb18-2"></a>    explan <span class="op">=</span> <span class="bu">sorted</span>(x[var])</span>
<span id="cb18-3"><a href="#cb18-3"></a>    preds <span class="op">=</span> []</span>
<span id="cb18-4"><a href="#cb18-4"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(explan)):</span>
<span id="cb18-5"><a href="#cb18-5"></a>        X_tmp <span class="op">=</span> x.copy()</span>
<span id="cb18-6"><a href="#cb18-6"></a>        <span class="co"># pandas is dumb</span></span>
<span id="cb18-7"><a href="#cb18-7"></a>        val <span class="op">=</span> np.asarray(explan)[i]</span>
<span id="cb18-8"><a href="#cb18-8"></a>        X_tmp[var] <span class="op">=</span> val</span>
<span id="cb18-9"><a href="#cb18-9"></a>        preds.append(model.predict(X_tmp))</span>
<span id="cb18-10"><a href="#cb18-10"></a>    preds <span class="op">=</span> np.asarray(preds).reshape(<span class="bu">len</span>(x), <span class="bu">len</span>(explan))</span>
<span id="cb18-11"><a href="#cb18-11"></a>    pv <span class="op">=</span> preds.mean(axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb18-12"><a href="#cb18-12"></a>    <span class="cf">return</span>(explan, pv)</span></code></pre></div>
<p>Next, lets scale that up to work over an entire data frame!</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="kw">def</span> pdp_df(model, x):</span>
<span id="cb19-2"><a href="#cb19-2"></a>    <span class="co"># return a dict of data frames</span></span>
<span id="cb19-3"><a href="#cb19-3"></a>    res <span class="op">=</span> {}</span>
<span id="cb19-4"><a href="#cb19-4"></a>    <span class="cf">for</span> c <span class="kw">in</span> x.columns:</span>
<span id="cb19-5"><a href="#cb19-5"></a>        d <span class="op">=</span> pd.DataFrame()</span>
<span id="cb19-6"><a href="#cb19-6"></a>        d[<span class="st">&quot;Value&quot;</span>], d[<span class="st">&quot;Average Prediction&quot;</span>] <span class="op">=</span> pdp_var(model, x, c)</span>
<span id="cb19-7"><a href="#cb19-7"></a>        res[c] <span class="op">=</span> d</span>
<span id="cb19-8"><a href="#cb19-8"></a>    <span class="cf">return</span>(res)</span></code></pre></div>
<p>Lets now show the partial dependence for just our random forest!</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a></span>
<span id="cb20-2"><a href="#cb20-2"></a>rf_pdp <span class="op">=</span> pdp_df(rf, X_test)</span>
<span id="cb20-3"><a href="#cb20-3"></a></span>
<span id="cb20-4"><a href="#cb20-4"></a>plt.style.use(<span class="st">&quot;seaborn-whitegrid&quot;</span>)</span>
<span id="cb20-5"><a href="#cb20-5"></a>fig <span class="op">=</span> plt.figure(figsize <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb20-6"><a href="#cb20-6"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(rf_pdp.keys())):</span>
<span id="cb20-7"><a href="#cb20-7"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>,<span class="dv">3</span>, k<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb20-8"><a href="#cb20-8"></a>    <span class="co">#c = cm.Paired(i/len(imps.keys()), 1)</span></span>
<span id="cb20-9"><a href="#cb20-9"></a>    c <span class="op">=</span> sns.color_palette(<span class="st">&quot;hls&quot;</span>, k<span class="op">+</span><span class="dv">1</span>)[k]</span>
<span id="cb20-10"><a href="#cb20-10"></a>    df <span class="op">=</span> rf_pdp[<span class="bu">list</span>(rf_pdp.keys())[k]]</span>
<span id="cb20-11"><a href="#cb20-11"></a>    sns.lineplot(x <span class="op">=</span> <span class="st">&quot;Value&quot;</span>, y <span class="op">=</span> <span class="st">&quot;Average Prediction&quot;</span>, ax <span class="op">=</span> ax, color <span class="op">=</span> c, data <span class="op">=</span> df)</span>
<span id="cb20-12"><a href="#cb20-12"></a>    ax.set_title(<span class="bu">list</span>(rf_pdp.keys())[k])</span>
<span id="cb20-13"><a href="#cb20-13"></a>plt.subplots_adjust(wspace <span class="op">=</span> <span class="fl">0.5</span>, hspace <span class="op">=</span> <span class="fl">0.5</span>)</span>
<span id="cb20-14"><a href="#cb20-14"></a>plt.show()</span></code></pre></div>
<p><img src="iml_files/figure-html/unnamed-chunk-14-1.svg" /><!-- --></p>
<p>Remember, we said that the variability in partial dependence is equivalent to the importance of a variable. Lets go ahead and write that up too!</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="kw">def</span> pdp_importance(model, x):</span>
<span id="cb21-2"><a href="#cb21-2"></a>    pdpdf <span class="op">=</span> pdp_df(model, x)</span>
<span id="cb21-3"><a href="#cb21-3"></a>    v <span class="op">=</span> {k:np.std(pdpdf[k][<span class="st">&quot;Average Prediction&quot;</span>]) <span class="cf">for</span> k <span class="kw">in</span> pdpdf.keys()}</span>
<span id="cb21-4"><a href="#cb21-4"></a>    <span class="cf">return</span>(v)</span>
<span id="cb21-5"><a href="#cb21-5"></a></span>
<span id="cb21-6"><a href="#cb21-6"></a></span>
<span id="cb21-7"><a href="#cb21-7"></a>pdp_imps <span class="op">=</span> {get_name(m):pdp_importance(m, X_test) <span class="cf">for</span> m <span class="kw">in</span> models}</span>
<span id="cb21-8"><a href="#cb21-8"></a></span>
<span id="cb21-9"><a href="#cb21-9"></a><span class="kw">def</span> plot_pdp_imp(d, ax <span class="op">=</span> <span class="va">None</span>, color <span class="op">=</span> <span class="st">&quot;blue&quot;</span>):</span>
<span id="cb21-10"><a href="#cb21-10"></a>    df <span class="op">=</span> pd.DataFrame()</span>
<span id="cb21-11"><a href="#cb21-11"></a>    df[<span class="st">&quot;Variable&quot;</span>] <span class="op">=</span> d.keys()</span>
<span id="cb21-12"><a href="#cb21-12"></a>    df[<span class="st">&quot;Importance&quot;</span>] <span class="op">=</span> d.values()</span>
<span id="cb21-13"><a href="#cb21-13"></a>    df.sort_values(<span class="st">&quot;Importance&quot;</span>).plot(kind <span class="op">=</span> <span class="st">&quot;barh&quot;</span>, x <span class="op">=</span> <span class="st">&quot;Variable&quot;</span>, y <span class="op">=</span> <span class="st">&quot;Importance&quot;</span>, width <span class="op">=</span> <span class="fl">0.8</span>, ax <span class="op">=</span> ax, color <span class="op">=</span> c)</span>
<span id="cb21-14"><a href="#cb21-14"></a></span>
<span id="cb21-15"><a href="#cb21-15"></a>fig <span class="op">=</span> plt.figure(figsize <span class="op">=</span> (<span class="dv">10</span>,<span class="dv">10</span>))</span>
<span id="cb21-16"><a href="#cb21-16"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(pdp_imps.keys())):</span>
<span id="cb21-17"><a href="#cb21-17"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="bu">len</span>(<span class="bu">list</span>(pdp_imps.keys())),<span class="dv">1</span>, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb21-18"><a href="#cb21-18"></a>    <span class="co">#c = cm.Paired(i/len(imps.keys()), 1)</span></span>
<span id="cb21-19"><a href="#cb21-19"></a>    c <span class="op">=</span> sns.color_palette(<span class="st">&quot;hls&quot;</span>, i<span class="op">+</span><span class="dv">1</span>)[i]</span>
<span id="cb21-20"><a href="#cb21-20"></a>    plot_pdp_imp(pdp_imps[<span class="bu">list</span>(pdp_imps.keys())[i]], ax <span class="op">=</span> ax, color <span class="op">=</span> c)</span>
<span id="cb21-21"><a href="#cb21-21"></a>    ax.set_title(<span class="bu">list</span>(pdp_imps.keys())[i])</span>
<span id="cb21-22"><a href="#cb21-22"></a>plt.show()</span></code></pre></div>
<p><img src="iml_files/figure-html/unnamed-chunk-15-1.svg" /><!-- --></p>
</div>
<div id="in-r-1" class="section level3">
<h3>In R</h3>
<p>First, lets define a function to calculate the partial dependence of a prediction on a single variable:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a>pdp_var &lt;-<span class="st"> </span><span class="cf">function</span>(model, x, var){</span>
<span id="cb22-2"><a href="#cb22-2"></a>  x_sorted &lt;-<span class="st"> </span>x[<span class="kw">order</span>(x[[var]], <span class="dt">decreasing =</span> <span class="ot">TRUE</span>),]</span>
<span id="cb22-3"><a href="#cb22-3"></a>  <span class="co"># predefine matrix!</span></span>
<span id="cb22-4"><a href="#cb22-4"></a>  out &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> <span class="kw">nrow</span>(x), <span class="dt">ncol =</span> <span class="kw">nrow</span>(x))</span>
<span id="cb22-5"><a href="#cb22-5"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(x)) {</span>
<span id="cb22-6"><a href="#cb22-6"></a>    tmp_df &lt;-<span class="st"> </span>x_sorted</span>
<span id="cb22-7"><a href="#cb22-7"></a>    tmp_df[[var]] &lt;-<span class="st"> </span>tmp_df[[var]][i]</span>
<span id="cb22-8"><a href="#cb22-8"></a>    out[i,] &lt;-<span class="st"> </span><span class="kw">predict</span>(model, tmp_df)</span>
<span id="cb22-9"><a href="#cb22-9"></a>  }</span>
<span id="cb22-10"><a href="#cb22-10"></a>  res &lt;-<span class="st"> </span><span class="kw">colMeans</span>(out)</span>
<span id="cb22-11"><a href="#cb22-11"></a>  <span class="kw">return</span>(<span class="kw">data.frame</span>(<span class="st">&quot;value&quot;</span> =<span class="st"> </span>x_sorted[[var]], <span class="st">&quot;avg_pred&quot;</span> =<span class="st"> </span>res))</span>
<span id="cb22-12"><a href="#cb22-12"></a>}</span></code></pre></div>
<p>Next lets scale that up over the entire data frame!</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a>pdp_df &lt;-<span class="st"> </span><span class="cf">function</span>(model, x) {</span>
<span id="cb23-2"><a href="#cb23-2"></a>pdps &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="kw">colnames</span>(x), <span class="cf">function</span>(n) {</span>
<span id="cb23-3"><a href="#cb23-3"></a>    res &lt;-<span class="st"> </span><span class="kw">pdp_var</span>(model, x, n)</span>
<span id="cb23-4"><a href="#cb23-4"></a>    res<span class="op">$</span>variable &lt;-<span class="st"> </span>n</span>
<span id="cb23-5"><a href="#cb23-5"></a>    <span class="kw">return</span>(res)</span>
<span id="cb23-6"><a href="#cb23-6"></a>  })</span>
<span id="cb23-7"><a href="#cb23-7"></a>  <span class="kw">return</span>(pdps)</span>
<span id="cb23-8"><a href="#cb23-8"></a>}</span>
<span id="cb23-9"><a href="#cb23-9"></a></span>
<span id="cb23-10"><a href="#cb23-10"></a>(rf_pdp &lt;-<span class="st"> </span><span class="kw">do.call</span>(rbind, <span class="kw">pdp_df</span>(trained_models[[<span class="dv">3</span>]], X_test)))</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["value"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["avg_pred"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["variable"],"name":[3],"type":["chr"],"align":["left"]}],"data":[{"1":"44.000","2":"8.829496","3":"Age"},{"1":"36.000","2":"3.238214","3":"Age"},{"1":"29.000","2":"5.547077","3":"Age"},{"1":"27.000","2":"1.804073","3":"Age"},{"1":"25.000","2":"5.135834","3":"Age"},{"1":"25.000","2":"6.591724","3":"Age"},{"1":"25.000","2":"9.277085","3":"Age"},{"1":"22.000","2":"5.951707","3":"Age"},{"1":"22.000","2":"9.224702","3":"Age"},{"1":"22.000","2":"1.064240","3":"Age"},{"1":"21.000","2":"3.737298","3":"Age"},{"1":"21.000","2":"5.213813","3":"Age"},{"1":"21.000","2":"3.347820","3":"Age"},{"1":"19.000","2":"1.937354","3":"Age"},{"1":"16.000","2":"8.897713","3":"Age"},{"1":"16.000","2":"5.952010","3":"Age"},{"1":"16.000","2":"2.202167","3":"Age"},{"1":"15.000","2":"3.812778","3":"Age"},{"1":"15.000","2":"8.210732","3":"Age"},{"1":"14.000","2":"10.517606","3":"Age"},{"1":"14.000","2":"3.397950","3":"Age"},{"1":"13.000","2":"10.766167","3":"Age"},{"1":"13.000","2":"9.645898","3":"Age"},{"1":"13.000","2":"9.120042","3":"Age"},{"1":"13.000","2":"3.812778","3":"Age"},{"1":"12.000","2":"9.645898","3":"Age"},{"1":"12.000","2":"1.145538","3":"Age"},{"1":"12.000","2":"11.374013","3":"Age"},{"1":"12.000","2":"4.800967","3":"Age"},{"1":"11.000","2":"9.645898","3":"Age"},{"1":"11.000","2":"9.959377","3":"Age"},{"1":"9.000","2":"10.996737","3":"Age"},{"1":"9.000","2":"9.013024","3":"Age"},{"1":"9.000","2":"9.047427","3":"Age"},{"1":"7.000","2":"9.495722","3":"Age"},{"1":"7.000","2":"12.209086","3":"Age"},{"1":"6.000","2":"3.689492","3":"Age"},{"1":"6.000","2":"11.996851","3":"Age"},{"1":"6.000","2":"9.955969","3":"Age"},{"1":"5.000","2":"12.606880","3":"Age"},{"1":"133.500","2":"11.188936","3":"Tonnage"},{"1":"116.000","2":"10.486241","3":"Tonnage"},{"1":"110.239","2":"10.130190","3":"Tonnage"},{"1":"108.865","2":"9.979543","3":"Tonnage"},{"1":"105.000","2":"10.163633","3":"Tonnage"},{"1":"101.509","2":"9.742248","3":"Tonnage"},{"1":"101.509","2":"9.929702","3":"Tonnage"},{"1":"93.000","2":"9.661838","3":"Tonnage"},{"1":"91.000","2":"9.273400","3":"Tonnage"},{"1":"91.000","2":"9.322931","3":"Tonnage"},{"1":"91.000","2":"9.377196","3":"Tonnage"},{"1":"90.090","2":"8.997597","3":"Tonnage"},{"1":"90.000","2":"9.707110","3":"Tonnage"},{"1":"89.600","2":"9.407101","3":"Tonnage"},{"1":"85.619","2":"9.083849","3":"Tonnage"},{"1":"81.000","2":"8.919795","3":"Tonnage"},{"1":"78.491","2":"8.336505","3":"Tonnage"},{"1":"77.713","2":"8.440029","3":"Tonnage"},{"1":"73.941","2":"8.737632","3":"Tonnage"},{"1":"73.192","2":"8.827074","3":"Tonnage"},{"1":"70.327","2":"8.552531","3":"Tonnage"},{"1":"59.652","2":"6.104204","3":"Tonnage"},{"1":"52.926","2":"6.201667","3":"Tonnage"},{"1":"50.000","2":"5.129168","3":"Tonnage"},{"1":"42.000","2":"6.628940","3":"Tonnage"},{"1":"38.000","2":"5.591015","3":"Tonnage"},{"1":"34.250","2":"5.583397","3":"Tonnage"},{"1":"33.920","2":"5.989487","3":"Tonnage"},{"1":"33.000","2":"4.305451","3":"Tonnage"},{"1":"30.277","2":"4.742059","3":"Tonnage"},{"1":"30.277","2":"4.908959","3":"Tonnage"},{"1":"30.277","2":"4.857823","3":"Tonnage"},{"1":"28.430","2":"4.877957","3":"Tonnage"},{"1":"19.200","2":"3.847015","3":"Tonnage"},{"1":"19.093","2":"4.810385","3":"Tonnage"},{"1":"16.852","2":"4.928937","3":"Tonnage"},{"1":"16.800","2":"3.778151","3":"Tonnage"},{"1":"12.500","2":"3.814961","3":"Tonnage"},{"1":"3.341","2":"3.323388","3":"Tonnage"},{"1":"2.329","2":"3.553886","3":"Tonnage"},{"1":"39.590","2":"11.860119","3":"passengers"},{"1":"37.000","2":"10.817015","3":"passengers"},{"1":"31.000","2":"11.340747","3":"passengers"},{"1":"28.520","2":"8.558305","3":"passengers"},{"1":"27.580","2":"9.978323","3":"passengers"},{"1":"27.580","2":"10.176962","3":"passengers"},{"1":"27.580","2":"10.738799","3":"passengers"},{"1":"27.440","2":"8.555195","3":"passengers"},{"1":"27.200","2":"10.472365","3":"passengers"},{"1":"25.500","2":"9.422320","3":"passengers"},{"1":"25.010","2":"9.136017","3":"passengers"},{"1":"24.350","2":"8.527094","3":"passengers"},{"1":"23.940","2":"9.817007","3":"passengers"},{"1":"22.400","2":"9.720497","3":"passengers"},{"1":"21.440","2":"8.894046","3":"passengers"},{"1":"21.140","2":"9.158017","3":"passengers"},{"1":"20.320","2":"9.401597","3":"passengers"},{"1":"20.320","2":"9.473540","3":"passengers"},{"1":"20.320","2":"9.501068","3":"passengers"},{"1":"18.820","2":"8.829372","3":"passengers"},{"1":"17.910","2":"8.817332","3":"passengers"},{"1":"15.040","2":"6.405664","3":"passengers"},{"1":"13.200","2":"5.985153","3":"passengers"},{"1":"13.020","2":"5.970475","3":"passengers"},{"1":"12.140","2":"5.553581","3":"passengers"},{"1":"10.560","2":"5.506387","3":"passengers"},{"1":"10.520","2":"5.477308","3":"passengers"},{"1":"9.520","2":"3.774138","3":"passengers"},{"1":"8.080","2":"4.413127","3":"passengers"},{"1":"8.000","2":"4.038796","3":"passengers"},{"1":"7.000","2":"5.453596","3":"passengers"},{"1":"6.940","2":"4.433926","3":"passengers"},{"1":"6.840","2":"4.514061","3":"passengers"},{"1":"6.840","2":"4.471276","3":"passengers"},{"1":"4.900","2":"4.509021","3":"passengers"},{"1":"3.940","2":"3.060865","3":"passengers"},{"1":"3.200","2":"3.563308","3":"passengers"},{"1":"2.960","2":"3.263231","3":"passengers"},{"1":"0.940","2":"2.991910","3":"passengers"},{"1":"0.660","2":"2.797939","3":"passengers"},{"1":"10.930","2":"11.362471","3":"length"},{"1":"9.650","2":"8.968264","3":"length"},{"1":"9.650","2":"9.123422","3":"length"},{"1":"9.650","2":"9.087154","3":"length"},{"1":"9.650","2":"9.345196","3":"length"},{"1":"9.650","2":"9.537926","3":"length"},{"1":"9.630","2":"8.329251","3":"length"},{"1":"9.620","2":"8.954834","3":"length"},{"1":"9.610","2":"9.418454","3":"length"},{"1":"9.570","2":"8.984161","3":"length"},{"1":"9.510","2":"11.405367","3":"length"},{"1":"9.510","2":"11.644943","3":"length"},{"1":"9.510","2":"11.032212","3":"length"},{"1":"9.210","2":"8.896093","3":"length"},{"1":"9.150","2":"8.385427","3":"length"},{"1":"8.930","2":"10.274315","3":"length"},{"1":"8.930","2":"10.552945","3":"length"},{"1":"8.900","2":"10.900672","3":"length"},{"1":"8.800","2":"8.818133","3":"length"},{"1":"8.800","2":"8.899326","3":"length"},{"1":"8.660","2":"8.528468","3":"length"},{"1":"7.770","2":"6.110661","3":"length"},{"1":"7.180","2":"6.210078","3":"length"},{"1":"7.090","2":"5.105091","3":"length"},{"1":"7.080","2":"6.630684","3":"length"},{"1":"7.040","2":"5.542391","3":"length"},{"1":"6.160","2":"4.415788","3":"length"},{"1":"6.150","2":"5.623644","3":"length"},{"1":"5.940","2":"4.403954","3":"length"},{"1":"5.940","2":"4.514524","3":"length"},{"1":"5.940","2":"4.429987","3":"length"},{"1":"5.670","2":"5.669388","3":"length"},{"1":"5.600","2":"4.244760","3":"length"},{"1":"5.410","2":"4.115239","3":"length"},{"1":"5.370","2":"4.187844","3":"length"},{"1":"5.140","2":"3.026203","3":"length"},{"1":"5.130","2":"3.256941","3":"length"},{"1":"4.360","2":"2.994099","3":"length"},{"1":"2.960","2":"2.663896","3":"length"},{"1":"2.790","2":"2.458739","3":"length"},{"1":"16.370","2":"10.692627","3":"cabins"},{"1":"15.570","2":"10.373399","3":"cabins"},{"1":"14.870","2":"10.204621","3":"cabins"},{"1":"13.790","2":"9.321058","3":"cabins"},{"1":"13.560","2":"9.768475","3":"cabins"},{"1":"13.210","2":"9.374589","3":"cabins"},{"1":"13.000","2":"10.215847","3":"cabins"},{"1":"12.750","2":"8.539904","3":"cabins"},{"1":"11.970","2":"9.220906","3":"cabins"},{"1":"11.770","2":"8.437598","3":"cabins"},{"1":"11.380","2":"8.451428","3":"cabins"},{"1":"11.200","2":"9.271991","3":"cabins"},{"1":"10.940","2":"8.578438","3":"cabins"},{"1":"10.720","2":"8.554181","3":"cabins"},{"1":"10.560","2":"8.750955","3":"cabins"},{"1":"10.000","2":"8.360607","3":"cabins"},{"1":"9.750","2":"9.222578","3":"cabins"},{"1":"9.750","2":"9.214377","3":"cabins"},{"1":"9.750","2":"9.237084","3":"cabins"},{"1":"9.500","2":"8.466364","3":"cabins"},{"1":"9.350","2":"8.620855","3":"cabins"},{"1":"7.520","2":"6.513054","3":"cabins"},{"1":"6.600","2":"6.296474","3":"cabins"},{"1":"6.540","2":"6.382022","3":"cabins"},{"1":"6.070","2":"6.062486","3":"cabins"},{"1":"5.280","2":"5.921436","3":"cabins"},{"1":"5.260","2":"5.829513","3":"cabins"},{"1":"4.100","2":"4.939888","3":"cabins"},{"1":"4.000","2":"4.680816","3":"cabins"},{"1":"3.830","2":"4.613772","3":"cabins"},{"1":"3.550","2":"5.085162","3":"cabins"},{"1":"3.540","2":"6.031261","3":"cabins"},{"1":"3.420","2":"5.106928","3":"cabins"},{"1":"3.420","2":"5.047627","3":"cabins"},{"1":"2.450","2":"5.051429","3":"cabins"},{"1":"1.600","2":"4.082177","3":"cabins"},{"1":"1.480","2":"3.835695","3":"cabins"},{"1":"0.880","2":"3.895368","3":"cabins"},{"1":"0.450","2":"3.771455","3":"cabins"},{"1":"0.330","2":"3.477653","3":"cabins"},{"1":"71.430","2":"4.864020","3":"passenger_density"},{"1":"67.350","2":"3.619405","3":"passenger_density"},{"1":"60.000","2":"2.287464","3":"passenger_density"},{"1":"56.760","2":"1.948244","3":"passenger_density"},{"1":"50.620","2":"1.027716","3":"passenger_density"},{"1":"45.190","2":"5.989604","3":"passenger_density"},{"1":"44.780","2":"9.455114","3":"passenger_density"},{"1":"44.780","2":"9.486575","3":"passenger_density"},{"1":"44.780","2":"9.526697","3":"passenger_density"},{"1":"44.260","2":"3.759362","3":"passenger_density"},{"1":"44.260","2":"3.688327","3":"passenger_density"},{"1":"42.640","2":"3.676134","3":"passenger_density"},{"1":"41.290","2":"8.681351","3":"passenger_density"},{"1":"40.650","2":"6.052969","3":"passenger_density"},{"1":"40.500","2":"9.067219","3":"passenger_density"},{"1":"40.180","2":"9.894675","3":"passenger_density"},{"1":"39.470","2":"11.513626","3":"passenger_density"},{"1":"39.270","2":"8.524561","3":"passenger_density"},{"1":"38.850","2":"9.965727","3":"passenger_density"},{"1":"38.600","2":"11.121221","3":"passenger_density"},{"1":"37.780","2":"9.178640","3":"passenger_density"},{"1":"37.420","2":"11.983588","3":"passenger_density"},{"1":"36.810","2":"10.570044","3":"passenger_density"},{"1":"36.810","2":"10.833387","3":"passenger_density"},{"1":"36.020","2":"9.400761","3":"passenger_density"},{"1":"35.980","2":"5.136267","3":"passenger_density"},{"1":"35.190","2":"3.729039","3":"passenger_density"},{"1":"35.140","2":"9.807575","3":"passenger_density"},{"1":"33.720","2":"12.720838","3":"passenger_density"},{"1":"32.560","2":"5.039148","3":"passenger_density"},{"1":"32.230","2":"8.852812","3":"passenger_density"},{"1":"31.730","2":"1.731165","3":"passenger_density"},{"1":"30.940","2":"9.283287","3":"passenger_density"},{"1":"29.790","2":"12.019904","3":"passenger_density"},{"1":"27.940","2":"5.332532","3":"passenger_density"},{"1":"27.930","2":"6.470887","3":"passenger_density"},{"1":"25.660","2":"9.343674","3":"passenger_density"},{"1":"24.780","2":"1.168108","3":"passenger_density"},{"1":"23.870","2":"3.349392","3":"passenger_density"},{"1":"17.700","2":"3.295743","3":"passenger_density"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Finally, lets plot these pdps!</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="kw">library</span>(ggthemes)</span>
<span id="cb24-2"><a href="#cb24-2"></a>rf_pdp <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">color =</span> variable, <span class="dt">y =</span>  avg_pred, <span class="dt">x =</span> value)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb24-3"><a href="#cb24-3"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.5</span>) <span class="op">+</span></span>
<span id="cb24-4"><a href="#cb24-4"></a><span class="st">  </span><span class="kw">facet_wrap</span>(variable <span class="op">~</span><span class="st"> </span>., <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span></span>
<span id="cb24-5"><a href="#cb24-5"></a><span class="st">  </span><span class="kw">theme_fivethirtyeight</span>() <span class="op">+</span><span class="st"> </span><span class="kw">scale_color_hc</span>()</span></code></pre></div>
<p><img src="iml_files/figure-html/unnamed-chunk-18-1.svg" width="960" /></p>
<p>Remember, we said that the variability in partial dependence is equivalent to the importance of a variable. Lets go ahead and write that up too! We are going to do a lot of <code>lapply</code>, as we are dealing with a list of lists of dataframes, and trying to quickly get that to a tidy format for ggplot</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="co"># return the pdp lists for all trained models</span></span>
<span id="cb25-2"><a href="#cb25-2"></a>pdps &lt;-<span class="st"> </span><span class="kw">lapply</span>(trained_models, <span class="cf">function</span>(m) <span class="kw">pdp_df</span>(m, X_test))</span>
<span id="cb25-3"><a href="#cb25-3"></a></span>
<span id="cb25-4"><a href="#cb25-4"></a><span class="co"># calculate the the standard deviation for average predictions</span></span>
<span id="cb25-5"><a href="#cb25-5"></a>sd_pdp &lt;-<span class="st"> </span><span class="cf">function</span>(df) {</span>
<span id="cb25-6"><a href="#cb25-6"></a>  imp &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">sd</span>(df<span class="op">$</span>avg_pred))</span>
<span id="cb25-7"><a href="#cb25-7"></a>  <span class="kw">return</span>(<span class="kw">data.frame</span>(<span class="st">&quot;importance&quot;</span> =<span class="st"> </span>imp, <span class="st">&quot;variable&quot;</span> =<span class="st"> </span>df<span class="op">$</span>variable[<span class="dv">2</span>]))</span>
<span id="cb25-8"><a href="#cb25-8"></a>}</span>
<span id="cb25-9"><a href="#cb25-9"></a></span>
<span id="cb25-10"><a href="#cb25-10"></a><span class="co"># we have a list of lists of data frames, so things are going to get a little weird</span></span>
<span id="cb25-11"><a href="#cb25-11"></a><span class="co"># We need to apply our function at depth of two, hence the double lapply</span></span>
<span id="cb25-12"><a href="#cb25-12"></a>pdp_imps &lt;-<span class="st"> </span><span class="kw">lapply</span>(pdps, <span class="cf">function</span>(x) <span class="kw">lapply</span>(x, sd_pdp))</span>
<span id="cb25-13"><a href="#cb25-13"></a></span>
<span id="cb25-14"><a href="#cb25-14"></a><span class="co"># next we need to clean up all the items of our lists are tidy data frames/matrices</span></span>
<span id="cb25-15"><a href="#cb25-15"></a>pdp_imps &lt;-<span class="st"> </span><span class="kw">lapply</span>(pdp_imps, <span class="cf">function</span>(x) <span class="kw">do.call</span>(rbind, x))</span>
<span id="cb25-16"><a href="#cb25-16"></a></span>
<span id="cb25-17"><a href="#cb25-17"></a><span class="co"># finally before we can plot, we add a character vector indicating model type</span></span>
<span id="cb25-18"><a href="#cb25-18"></a>pdp_imps &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="kw">names</span>(pdp_imps), <span class="cf">function</span>(x) {</span>
<span id="cb25-19"><a href="#cb25-19"></a>  pdp_imps[[x]]<span class="op">$</span>model &lt;-<span class="st"> </span>x</span>
<span id="cb25-20"><a href="#cb25-20"></a>  <span class="kw">return</span>(pdp_imps[[x]])</span>
<span id="cb25-21"><a href="#cb25-21"></a>  }</span>
<span id="cb25-22"><a href="#cb25-22"></a>)</span>
<span id="cb25-23"><a href="#cb25-23"></a></span>
<span id="cb25-24"><a href="#cb25-24"></a><span class="co"># then we turn it all into a nice data frame</span></span>
<span id="cb25-25"><a href="#cb25-25"></a>pdp_imps &lt;-<span class="st"> </span><span class="kw">do.call</span>(rbind, pdp_imps)</span>
<span id="cb25-26"><a href="#cb25-26"></a></span>
<span id="cb25-27"><a href="#cb25-27"></a>pdp_imps<span class="op">%&gt;%</span><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> variable, <span class="dt">y =</span> importance, <span class="dt">fill =</span> model)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="op">+</span></span>
<span id="cb25-28"><a href="#cb25-28"></a><span class="st">  </span><span class="kw">facet_grid</span>(model <span class="op">~</span><span class="st"> </span>., <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">coord_flip</span>() <span class="op">+</span><span class="st"> </span>ggthemes<span class="op">::</span><span class="kw">theme_fivethirtyeight</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb25-29"><a href="#cb25-29"></a><span class="st">  </span>ggthemes<span class="op">::</span><span class="kw">scale_fill_fivethirtyeight</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Partial Dependence Importance&quot;</span>)</span></code></pre></div>
<p><img src="iml_files/figure-html/unnamed-chunk-19-1.svg" width="672" /></p>
</div>
</div>
<div id="discussion" class="section level2">
<h2>Discussion</h2>
<p>Something is off!!!</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
